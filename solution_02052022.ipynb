{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm, json, pickle, gc, zipfile, itertools, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, ParameterGrid, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tqdm.contrib.concurrent import process_map  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap \n",
    "from sklearn.model_selection import KFold\n",
    "from nancorrmp.nancorrmp import NaNCorrMp\n",
    "# from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "class CsgoOutcomePredictor():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "    \n",
    "    def get_game_collection(self, PATH_TO_DIR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Описание: коллекционирование респонсов парсера\n",
    "        Параметры: PATH_TO_DIR - путь до директории с респонсами\n",
    "        \"\"\"\n",
    "\n",
    "        L_FILENAMES = os.listdir(PATH_TO_DIR)\n",
    "        L_COLLECTION = []\n",
    "        for fnm in tqdm.tqdm(L_FILENAMES):\n",
    "            try:\n",
    "                pth = os.path.join(PATH_TO_DIR, fnm)\n",
    "                with open(pth, 'r') as f:\n",
    "                    d_rsp = json.load(f)\n",
    "                L_COLLECTION.append(d_rsp)\n",
    "            except:\n",
    "                pass\n",
    "        idx_ordered = np.argsort([d_game['id'] for d_game in L_COLLECTION])[::-1]\n",
    "        L_COLLECTION = np.array(L_COLLECTION)[idx_ordered].tolist()\n",
    "        return L_COLLECTION\n",
    "\n",
    "    def add_global_info(self, d_game):\n",
    "\n",
    "        d = {}\n",
    "\n",
    "        d['id'] = d_game['id']\n",
    "        d['match_id'] = d_game['match_id']\n",
    "        d['match_type'] = d_game['match']['match_type']\n",
    "        d['number_of_games'] = d_game['match']['number_of_games']\n",
    "        d['date'] = parser.parse(d_game['begin_at'])\n",
    "        d['map_id'] = d_game['map']['id']\n",
    "        d['league_id'] = d_game['match']['league']['id']\n",
    "        d['serie_id'] = d_game['match']['serie']['id']\n",
    "        d['tournament_id'] = d_game['match']['tournament']['id']\n",
    "        d['serie_tier'] = d_game['match']['serie']['tier']\n",
    "\n",
    "        return d\n",
    "\n",
    "    def add_profile(self, d_game):\n",
    "            \n",
    "        # идентификаторы актуальных карт\n",
    "        l_map2use = [1, 2, 6, 7, 8, 20, 31]\n",
    "        # ключи со статистикой игрока\n",
    "        l_stat_keys = ['adr', 'assists', 'deaths', 'first_kills_diff', 'flash_assists', \n",
    "                    'headshots', 'k_d_diff', 'kast', 'kills', 'rating']\n",
    "\n",
    "        # информация об игре\n",
    "        d_info = self.add_global_info(d_game)\n",
    "        \n",
    "        if d_info['map_id'] in l_map2use:  \n",
    "\n",
    "            d_r1 = d_game['rounds'][0]\n",
    "            if d_r1['round']==1:\n",
    "                \n",
    "                # информация о раундах\n",
    "                df_rounds = pd.DataFrame.from_records(d_game['rounds'])\n",
    "                start_ct_id =d_r1['ct']   \n",
    "                winner_id = df_rounds['winner_team'].value_counts().idxmax()\n",
    "                maxround = df_rounds['round'].max()\n",
    "                d_h1_win_count = df_rounds.query('round<=15')['winner_team'].value_counts().to_dict()\n",
    "                d_h2_win_count = df_rounds.query('round>15')['winner_team'].value_counts().to_dict()\n",
    "                d_h1_outcome_count = df_rounds.query('round<=15')['outcome'].value_counts().to_dict()\n",
    "                d_h2_outcome_count = df_rounds.query('round>15')['outcome'].value_counts().to_dict()        \n",
    "\n",
    "                L = []\n",
    "                counter = 0\n",
    "                # информация об игроках\n",
    "                for p in d_game['players']:\n",
    "                    counter+=1\n",
    "\n",
    "                    d = {}\n",
    "                    d.update(d_info)\n",
    "\n",
    "                    # идентификатор игрока\n",
    "                    d['player_id'] = p['player']['id']\n",
    "                    # идентификатор команды\n",
    "                    d['team_id'] = p['team']['id']\n",
    "                    # идентификатор оппонента\n",
    "                    d['opponent_id'] = p['opponent']['id']\n",
    "\n",
    "                    # национальность игрока\n",
    "                    d['player_nationality']  = p['player']['nationality']\n",
    "                    # дата рождения игрока\n",
    "                    d['player_birthday']  = p['player']['birthday']\n",
    "                    # страна команды\n",
    "                    d['team_location']  = p['team']['location']\n",
    "\n",
    "                    # сторона начала\n",
    "                    d['start_ct']= 1 if start_ct_id==d['team_id'] else 0\n",
    "                    # победа\n",
    "                    d['win'] = 1 if winner_id==d['team_id'] else 0\n",
    "                    # все раундов в игре\n",
    "                    d['maxround'] = maxround\n",
    "\n",
    "                    # число выигранных раундов в 1-ой половине игры\n",
    "                    try:\n",
    "                        d['h1_win_count'] = d_h1_win_count[d['team_id']]\n",
    "                    except:\n",
    "                        d['h1_win_count'] = 0 \n",
    "                    # число выигранных раундов во 2-ой половине игры\n",
    "                    try:\n",
    "                        d['h2_win_count'] = d_h2_win_count[d['team_id']]\n",
    "                    except:\n",
    "                        d['h2_win_count'] = 0 \n",
    "                    # исходы раундов в 1-ой половине игры\n",
    "                    for k, v in d_h1_outcome_count.items():\n",
    "                        d[f'h1_outcome_{k}_count'] = v\n",
    "                    # исходы раундов во 2-ой половине игры\n",
    "                    for k, v in d_h2_outcome_count.items():\n",
    "                        d[f'h2_outcome_{k}_count'] = v            \n",
    "\n",
    "                    # статистика игрока\n",
    "                    d.update({k:p[k] if pd.notnull(p[k]) else 0.0 for k in l_stat_keys})\n",
    "                    d.update({f'{k}_per_round':p[k]/maxround if pd.notnull(p[k]) else 0.0 for k in l_stat_keys})\n",
    "\n",
    "                    L.append(d)\n",
    "                if counter==10:\n",
    "                    return L\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def get_profiles(self, L_COLLECTION):\n",
    "\n",
    "        \"\"\"\n",
    "        Описание: профайлинг игроков в играх\n",
    "        Параметры: L_COLLECTION- коллекция респонсов\n",
    "        \"\"\"        \n",
    "            \n",
    "        # информация об игре\n",
    "        L_GLOBAL_KEYS = [\n",
    "            'id', 'match_id', 'match_type', 'number_of_games',\n",
    "            'date', 'year', 'month', 'day', 'weekday', 'hour',\n",
    "            'map_id',\n",
    "            'league_id', 'serie_id', 'tournament_id', 'serie_tier',\n",
    "            'start_ct'\n",
    "        ]\n",
    "        # ключи для агрегирования\n",
    "        L_AGG_KEYS = [    \n",
    "            \n",
    "            'h1_outcome_defused_count', 'h1_outcome_eliminated_count',\n",
    "            'h1_outcome_exploded_count', 'h1_outcome_timeout_count',\n",
    "            'h1_win_count', 'h2_outcome_defused_count',\n",
    "            'h2_outcome_eliminated_count', 'h2_outcome_exploded_count',\n",
    "            'h2_outcome_timeout_count', 'h2_win_count',\n",
    "\n",
    "            'win', 'maxround',\n",
    "\n",
    "            'adr', 'assists', 'deaths', 'first_kills_diff', 'flash_assists', 'headshots',\n",
    "            'k_d_diff', 'kast', 'kills', 'rating', \n",
    "            'adr_per_round', 'assists_per_round', 'deaths_per_round', 'first_kills_diff_per_round', 'flash_assists_per_round', 'headshots_per_round',\n",
    "            'k_d_diff_per_round', 'kast_per_round', 'kills_per_round','rating_per_round'\n",
    "        ]\n",
    "        # ключи для группировки\n",
    "        L_GROUP_KEYS = [\n",
    "            'team_id', 'opponent_id', 'team_location', 'lineup'\n",
    "        ]\n",
    "\n",
    "        # профайлинг игрока\n",
    "        L_player_profile = []\n",
    "        for d_game in tqdm.tqdm(L_COLLECTION):\n",
    "            try:\n",
    "                L_player_profile.extend(self.add_profile(d_game))        \n",
    "            except:\n",
    "                pass\n",
    "        df_player_profile = pd.DataFrame.from_records(L_player_profile)\n",
    "        del L_player_profile\n",
    "        gc.collect()\n",
    "\n",
    "        L_dict = []\n",
    "        for (game_id, team_id), subdf in tqdm.tqdm(df_player_profile.groupby(['id', 'team_id'])):\n",
    "            n_players = subdf.shape[0]\n",
    "            if n_players==5:\n",
    "                subdf_c = subdf.copy()\n",
    "                lineup = '-'.join(subdf['player_id'].sort_values().astype(str))\n",
    "                subdf_c['lineup'] = lineup\n",
    "                L_dict.extend(subdf_c.to_dict('records'))\n",
    "        del df_player_profile\n",
    "        gc.collect()\n",
    "        df_player_profile = pd.DataFrame.from_records(L_dict).sort_values('date')\n",
    "        del L_dict\n",
    "        gc.collect()\n",
    "\n",
    "        date = df_player_profile['date']\n",
    "        df_player_profile['year'] = date.dt.year\n",
    "        df_player_profile['month'] = date.dt.month\n",
    "        df_player_profile['day'] = date.dt.day\n",
    "        df_player_profile['weekday'] = date.dt.weekday\n",
    "        df_player_profile['hour'] = date.dt.hour\n",
    "        df_player_profile[['serie_tier', 'team_location']] = df_player_profile[['serie_tier', 'team_location']].fillna('default')    \n",
    "\n",
    "        # профайлинг команды\n",
    "        L_team_profile = []\n",
    "        for (game_id, team_id), subdf in tqdm.tqdm(df_player_profile.groupby(['id', 'team_id'])):    \n",
    "            d = subdf[L_GLOBAL_KEYS+L_GROUP_KEYS].iloc[0].to_dict()    \n",
    "            d.update(subdf[L_AGG_KEYS].mean().to_dict())\n",
    "            L_team_profile.append(d)\n",
    "        df_team_profile = pd.DataFrame.from_records(L_team_profile)\n",
    "        del L_team_profile\n",
    "        gc.collect()\n",
    "\n",
    "        df_player_profile_c = df_player_profile.apply(self.reduce_mem_usage)\n",
    "        del df_player_profile \n",
    "        df_team_profile_c = df_team_profile.apply(self.reduce_mem_usage)\n",
    "        del df_team_profile \n",
    "\n",
    "        return df_player_profile_c, df_team_profile_c\n",
    "\n",
    "    def add_info4game(self, game_id):  \n",
    "\n",
    "        L_GAMEINFO_KEYS = [\n",
    "            'id',\n",
    "            'number_of_games',\n",
    "            'year','month', 'day', 'weekday', 'hour',\n",
    "            'map_id',\n",
    "            'league_id', 'serie_id', 'tournament_id', \n",
    "            'serie_tier'\n",
    "        ]\n",
    "        \n",
    "        df_game = self.df_team_profile.query('id==@game_id')\n",
    "\n",
    "        d_fs4gm = {}\n",
    "        d_fs4gm.update(df_game[L_GAMEINFO_KEYS].iloc[0].to_dict())\n",
    "\n",
    "        d_team_id2start_ct = dict(zip(df_game['team_id'], df_game['start_ct']))\n",
    "        d_team_id2opponent_id = dict(zip(df_game['team_id'], df_game['opponent_id']))\n",
    "        d_team_id2lineup = dict(zip(df_game['team_id'], df_game['lineup']))\n",
    "        d_team_id2loc = dict(zip(df_game['team_id'], df_game['team_location']))\n",
    "\n",
    "        df_game = self.df_player_profile.query('id==@game_id')\n",
    "\n",
    "        for team_id, subdf in df_game.groupby('team_id'):\n",
    "\n",
    "            prefix = 'start_ct' if d_team_id2start_ct[team_id]==1 else 'start_t'\n",
    "\n",
    "            d_fs4gm[f'{prefix}__team_id'] = team_id    \n",
    "            d_fs4gm[f'{prefix}__team_lineup'] = d_team_id2lineup[team_id]\n",
    "            d_fs4gm[f'{prefix}__team_location'] = d_team_id2loc[team_id]\n",
    "            \n",
    "            subdf = subdf.sort_values('player_id')    \n",
    "            L_p_id = subdf['player_id'].values    \n",
    "            d_player_id2nat = dict(zip(subdf['player_id'], subdf['player_nationality']))\n",
    "            ser_bd = subdf['player_birthday'].astype('datetime64')\n",
    "            ser_bd_y = ser_bd.dt.year\n",
    "            ser_bd_m = ser_bd.dt.month\n",
    "            ser_bd_d = ser_bd.dt.day\n",
    "\n",
    "            for i, p_id in enumerate(L_p_id):\n",
    "                d_fs4gm[f'{prefix}__player{i+1}_id'] = p_id\n",
    "                d_fs4gm[f'{prefix}__player{i+1}_nationality'] = d_player_id2nat[p_id]\n",
    "                d_fs4gm[f'{prefix}__player{i+1}_birthday_year'] = ser_bd_y.iloc[i]\n",
    "                d_fs4gm[f'{prefix}__player{i+1}_birthday_month'] = ser_bd_m.iloc[i]\n",
    "                d_fs4gm[f'{prefix}__player{i+1}_birthday_day'] = ser_bd_d.iloc[i]  \n",
    "        return d_fs4gm\n",
    "\n",
    "    def add_features__gameinfo(self, PATH_TO_GAMEINFO_FEATURES):        \n",
    "\n",
    "        ls = os.listdir(PATH_TO_GAMEINFO_FEATURES)\n",
    "        L_GAME_IDXS = np.unique(self.df_team_profile['id'])\n",
    "        try:\n",
    "            set_in = set([int(x.split('.')[0]) for x in ls])\n",
    "        except:\n",
    "            set_in = set()\n",
    "        set_all = set(L_GAME_IDXS)\n",
    "        set_new = set_all-set_in\n",
    "        L_GAME_IDXS = list(set_new)[::-1]\n",
    "\n",
    "        for game_id in tqdm.tqdm(L_GAME_IDXS):\n",
    "            try:    \n",
    "                d_fs4gm = self.add_info4game(game_id)\n",
    "                pth = os.path.join(PATH_TO_GAMEINFO_FEATURES, '{}.pickle'.format(game_id))\n",
    "                with open(pth, 'wb') as f:\n",
    "                    pickle.dump(d_fs4gm, f)\n",
    "                del d_fs4gm\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def add_features__team4game(self, game_id):  \n",
    "\n",
    "        L_GROUP_KEYS = [        \n",
    "            'number_of_games',\n",
    "            'year','month', 'day', 'weekday', 'hour',\n",
    "            'serie_tier'\n",
    "        ]\n",
    "        L_FILTER_KEYS = [\n",
    "            'league_id', 'serie_id', 'tournament_id'\n",
    "        ]\n",
    "\n",
    "        # ключи для агрегирования\n",
    "        L_AGG_KEYS = [  \n",
    "\n",
    "            'maxround', 'win', \n",
    "            \n",
    "            'h1_outcome_defused_count', 'h1_outcome_eliminated_count',\n",
    "            'h1_outcome_exploded_count', 'h1_outcome_timeout_count',\n",
    "            'h1_win_count', 'h2_outcome_defused_count',\n",
    "            'h2_outcome_eliminated_count', 'h2_outcome_exploded_count',\n",
    "            'h2_outcome_timeout_count', 'h2_win_count',    \n",
    "\n",
    "            'adr', 'first_kills_diff', 'k_d_diff', 'kast','rating', \n",
    "            'assists_per_round', 'deaths_per_round',\n",
    "            'flash_assists_per_round', 'headshots_per_round',\n",
    "            'kills_per_round'\n",
    "            \n",
    "        ]\n",
    "\n",
    "        L_BY_KEYS = [\n",
    "            'number_of_games',\n",
    "            'year','month', 'day', 'weekday', 'hour',\n",
    "            'serie_tier'\n",
    "        ]\n",
    "        \n",
    "        df_game = self.df_team_profile.query('id==@game_id')\n",
    "\n",
    "        date = df_game['date'].iloc[0]\n",
    "        map_id = df_game['map_id'].iloc[0]\n",
    "        league_id = df_game['league_id'].iloc[0]\n",
    "        serie_id = df_game['serie_id'].iloc[0]\n",
    "        tournament_id = df_game['tournament_id'].iloc[0]\n",
    "        d_filter = dict(zip(['league_id', 'serie_id', 'tournament_id'], [league_id, serie_id, tournament_id]))\n",
    "\n",
    "        d_fs4gm = {'id':game_id}    \n",
    "\n",
    "        d_team_id2start_ct = dict(zip(df_game['team_id'], df_game['start_ct']))\n",
    "        d_team_id2opponent_id = dict(zip(df_game['team_id'], df_game['opponent_id']))\n",
    "        d_team_id2lineup = dict(zip(df_game['team_id'], df_game['lineup']))\n",
    "        d_team_id2loc = dict(zip(df_game['team_id'], df_game['team_location']))\n",
    "        \n",
    "\n",
    "        for team_id, start_ct in d_team_id2start_ct.items():\n",
    "\n",
    "            opponent_id = d_team_id2opponent_id[team_id]\n",
    "            lineup = d_team_id2lineup[team_id]\n",
    "\n",
    "            prefix = 'start_ct' if start_ct==1 else 'start_t'\n",
    "\n",
    "            df_history = self.df_team_profile.query('(date<@date)&(team_id==@team_id)')\n",
    "            df_history_on_map_with_start = df_history.query('(map_id==@map_id)&(start_ct==@start_ct)')        \n",
    "            df_history_with_lineup = df_history.query('lineup==@lineup')\n",
    "            df_history_on_map_with_start_and_lineup = df_history.query('(map_id==@map_id)&(start_ct==@start_ct)&(lineup==@lineup)')\n",
    "            df_history_pair = df_history.query('opponent_id==@opponent_id')\n",
    "            df_history_on_map_with_start_and_pair = df_history.query('(map_id==@map_id)&(start_ct==@start_ct)&(opponent_id==@opponent_id)')\n",
    "\n",
    "            L_DF = [\n",
    "                df_history, df_history_on_map_with_start, \n",
    "                df_history_with_lineup, df_history_on_map_with_start_and_lineup,\n",
    "                df_history_pair, df_history_on_map_with_start_and_pair\n",
    "            ]\n",
    "            L_SUFFIX = [\n",
    "                'all_map_all_start', 'current_map_current_start', \n",
    "                'all_map_all_start__lineup', 'current_map_current_start__lineup',\n",
    "                'all_map_all_start__pair', 'current_map_current_start__pair',\n",
    "            ]\n",
    "\n",
    "            for filter_key, filter_value in d_filter.items():\n",
    "                for suffix, df in zip(['all_map_all_start', 'current_map_current_start'],\n",
    "                                    [df_history, df_history_on_map_with_start, ]):\n",
    "                    L_SUFFIX.append(filter_key)\n",
    "                    L_DF.append(df[df[filter_key]==filter_value])\n",
    "\n",
    "            d_dicts4team = dict(zip(L_SUFFIX, L_DF))\n",
    "            del L_SUFFIX, L_DF\n",
    "\n",
    "            \n",
    "            for suffix, subdf in d_dicts4team.items():                       \n",
    "                for key in L_AGG_KEYS:\n",
    "                    values = subdf[key].values\n",
    "                    d_fs4gm[f'{prefix}__team__{suffix}__{key}__mean'] = np.mean(values)\n",
    "                    d_fs4gm[f'{prefix}__team__{suffix}__{key}__sum'] = np.sum(values)\n",
    "                    for by_key in L_BY_KEYS:\n",
    "                        for by_value, subsubdf in subdf.groupby(by_key):\n",
    "                            values = subsubdf[key].values\n",
    "                            try:\n",
    "                                d_fs4gm[f'{prefix}__team__{suffix}__{by_key}_{int(by_value)}__{key}__mean'] = np.mean(values)\n",
    "                                d_fs4gm[f'{prefix}__team__{suffix}__{by_key}_{int(by_value)}__{key}__sum'] = np.sum(values)\n",
    "                            except:\n",
    "                                d_fs4gm[f'{prefix}__team__{suffix}__{by_key}_{by_value}__{key}__mean'] = np.mean(values)\n",
    "                                d_fs4gm[f'{prefix}__team__{suffix}__{by_key}_{by_value}__{key}__sum'] = np.sum(values)\n",
    "            del d_dicts4team\n",
    "\n",
    "        return d_fs4gm   \n",
    "\n",
    "    def add_features__team(self, PATH_TO_FEATURES_TEAM): \n",
    "        ls = os.listdir(PATH_TO_FEATURES_TEAM)\n",
    "        L_GAME_IDXS = np.unique(self.df_team_profile['id'])\n",
    "        try:\n",
    "            set_in = set([int(x.split('.')[0]) for x in ls])\n",
    "        except:\n",
    "            set_in = set()\n",
    "        set_all = set(L_GAME_IDXS)\n",
    "        set_new = set_all-set_in\n",
    "        L_GAME_IDXS = list(set_new)[::-1]\n",
    "\n",
    "        for game_id in tqdm.tqdm(L_GAME_IDXS):\n",
    "            try:    \n",
    "                d_fs4gm = self.add_features__team4game(game_id)\n",
    "                pth = os.path.join(PATH_TO_FEATURES_TEAM, '{}.pickle'.format(game_id))\n",
    "                with open(pth, 'wb') as f:\n",
    "                    pickle.dump(d_fs4gm, f)\n",
    "                del d_fs4gm\n",
    "            except:\n",
    "                pass   \n",
    "\n",
    "    def add_features__player4game(self, game_id):  \n",
    "\n",
    "        L_GROUP_KEYS = [        \n",
    "            'number_of_games',\n",
    "            'year','month', 'day', 'weekday', 'hour',\n",
    "            'serie_tier'\n",
    "        ]\n",
    "        L_FILTER_KEYS = [\n",
    "            'league_id', 'serie_id', 'tournament_id'\n",
    "        ]\n",
    "\n",
    "        # ключи для агрегирования\n",
    "        L_AGG_KEYS = [  \n",
    "            \n",
    "            'adr', 'first_kills_diff', 'k_d_diff', 'kast', 'rating', \n",
    "            'assists_per_round', 'deaths_per_round',\n",
    "            'flash_assists_per_round', 'headshots_per_round',\n",
    "            'kills_per_round'\n",
    "            \n",
    "        ]\n",
    "\n",
    "        L_BY_KEYS = [            \n",
    "            'year','month', 'day', 'weekday', 'hour'\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        df_game = self.df_player_profile.query('id==@game_id')\n",
    "\n",
    "        date = df_game['date'].iloc[0]\n",
    "        map_id = df_game['map_id'].iloc[0]\n",
    "        league_id = df_game['league_id'].iloc[0]\n",
    "        serie_id = df_game['serie_id'].iloc[0]\n",
    "        tournament_id = df_game['tournament_id'].iloc[0]\n",
    "        d_filter = dict(zip(['league_id', 'serie_id', 'tournament_id'], [league_id, serie_id, tournament_id]))\n",
    "\n",
    "        d_fs4gm = {'id':game_id}    \n",
    "\n",
    "        d_team_id2start_ct = dict(zip(df_game['team_id'], df_game['start_ct']))   \n",
    "\n",
    "        for team_id, start_ct in d_team_id2start_ct.items():        \n",
    "\n",
    "            prefix = 'start_ct' if start_ct==1 else 'start_t'\n",
    "\n",
    "            L_p_id = np.unique(df_game.query('team_id==@team_id')['player_id'])\n",
    "\n",
    "            for i, p_id in enumerate(L_p_id):\n",
    "\n",
    "                df_in_team_history = self.df_player_profile.query('(date<@date)&(player_id==@p_id)&(team_id==@team_id)')\n",
    "                df_in_team_history_on_map_with_start = df_in_team_history.query('(map_id==@map_id)&(start_ct==@start_ct)')  \n",
    "                df_not_in_team_history = self.df_player_profile.query('(date<@date)&(player_id==@p_id)&(team_id!=@team_id)')\n",
    "                df_not_in_team_history_on_map_with_start = df_in_team_history.query('(map_id==@map_id)&(start_ct==@start_ct)') \n",
    "\n",
    "                L_DF = [\n",
    "                    df_in_team_history, df_in_team_history_on_map_with_start, \n",
    "                    df_not_in_team_history, df_not_in_team_history_on_map_with_start                \n",
    "                ]\n",
    "                L_SUFFIX = [\n",
    "                    f'player{i+1}__in_team__all_map_all_start', f'player{i+1}__in_team__current_map_current_start', \n",
    "                    f'player{i+1}__not_in_team__all_map_all_start', f'player{i+1}__not_in_team__current_map_current_start', \n",
    "                ]        \n",
    "\n",
    "                d_dicts4player = dict(zip(L_SUFFIX, L_DF))\n",
    "                del L_SUFFIX, L_DF\n",
    "\n",
    "            \n",
    "                for suffix, subdf in d_dicts4player.items():                       \n",
    "                    for key in L_AGG_KEYS:\n",
    "                        values = subdf[key].values\n",
    "                        d_fs4gm[f'{prefix}__{suffix}__{key}__mean'] = np.mean(values)\n",
    "                        d_fs4gm[f'{prefix}__{suffix}__{key}__sum'] = np.sum(values)\n",
    "                        for by_key in L_BY_KEYS:\n",
    "                            for by_value, subsubdf in subdf.groupby(by_key):\n",
    "                                values = subsubdf[key].values\n",
    "                                try:\n",
    "                                    d_fs4gm[f'{prefix}__{suffix}__{by_key}_{int(by_value)}__{key}__mean'] = np.mean(values)\n",
    "                                    d_fs4gm[f'{prefix}__{suffix}__{by_key}_{int(by_value)}__{key}__sum'] = np.sum(values)\n",
    "                                except:\n",
    "                                    d_fs4gm[f'{prefix}__{suffix}__{by_key}_{by_value}__{key}__mean'] = np.mean(values)\n",
    "                                    d_fs4gm[f'{prefix}__{suffix}__{by_key}_{by_value}__{key}__sum'] = np.sum(values)\n",
    "                del d_dicts4player\n",
    "\n",
    "        return d_fs4gm\n",
    "\n",
    "    def add_features__player(self, PATH_TO_FEATURES_PLAYER): \n",
    "        ls = os.listdir(PATH_TO_FEATURES_PLAYER)\n",
    "        L_GAME_IDXS = np.unique(self.df_player_profile['id'])\n",
    "        try:\n",
    "            set_in = set([int(x.split('.')[0]) for x in ls])\n",
    "        except:\n",
    "            set_in = set()\n",
    "        set_all = set(L_GAME_IDXS)\n",
    "        set_new = set_all-set_in\n",
    "        L_GAME_IDXS = list(set_new)[::-1]\n",
    "\n",
    "        for game_id in tqdm.tqdm(L_GAME_IDXS):\n",
    "            try:    \n",
    "                d_fs4gm = self.add_features__player4game(game_id)\n",
    "                pth = os.path.join(PATH_TO_FEATURES_PLAYER, '{}.pickle'.format(game_id))\n",
    "                with open(pth, 'wb') as f:\n",
    "                    pickle.dump(d_fs4gm, f)\n",
    "                del d_fs4gm\n",
    "            except:\n",
    "                pass  \n",
    "\n",
    "    def reduce_mem_usage(self, series):\n",
    "        try:\n",
    "            col_type = series.dtype\n",
    "\n",
    "            if col_type != object:\n",
    "                c_min = series.min()\n",
    "                c_max = series.max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        series = series.astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        series = series.astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        series = series.astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        series = series.astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        series = series.astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        series = series.astype(np.float32)\n",
    "                    else:\n",
    "                        series = series.astype(np.float64)\n",
    "            else:\n",
    "                pass \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return series \n",
    "\n",
    "    def build_features(self, PATH_TO_FEATURES_GAMEINFO, PATH_TO_FEATURES_TEAM, PATH_TO_FEATURES_PLAYER):\n",
    "\n",
    "        \"\"\"\n",
    "        Сборка признаков\n",
    "        \"\"\"   \n",
    "\n",
    "        # все файлы с признаками\n",
    "        set_gameinfo= set(os.listdir(PATH_TO_FEATURES_GAMEINFO))\n",
    "        set_team= set(os.listdir(PATH_TO_FEATURES_TEAM))\n",
    "        set_player= set(os.listdir(PATH_TO_FEATURES_PLAYER))\n",
    "        l_all_files = np.array(list(set.intersection(*[set_gameinfo, set_team, set_player])))\n",
    "        l_all_files = l_all_files[np.argsort([int(x.split('.')[0]) for x in l_all_files])]\n",
    "\n",
    "        # размер батча\n",
    "        batch_size = 100\n",
    "        n = np.int32(np.ceil(len(l_all_files) / batch_size))\n",
    "        l_batches = np.array_split(l_all_files, n)\n",
    "\n",
    "        # сборка\n",
    "        df_features = pd.DataFrame()\n",
    "        for batch in tqdm.tqdm(l_batches[-250:]):\n",
    "            \n",
    "            l = []\n",
    "            for fnm in batch:\n",
    "                D = {}\n",
    "                for pth2dir in [PATH_TO_FEATURES_GAMEINFO, PATH_TO_FEATURES_TEAM, PATH_TO_FEATURES_PLAYER]:\n",
    "                    pth = os.path.join(pth2dir, fnm)\n",
    "                    with open(pth, 'rb') as f:\n",
    "                        d = pickle.load(f)\n",
    "                    D.update(d)\n",
    "                    del d\n",
    "                l.append(D)\n",
    "                del D\n",
    "            \n",
    "            df = pd.DataFrame.from_records(l).apply(self.reduce_mem_usage)\n",
    "            del l\n",
    "            df_features = df_features.append(df)\n",
    "            del df\n",
    "            gc.collect()\n",
    "        \n",
    "        return df_features\n",
    "\n",
    "    def build_targets(self, L_GAME_IDXS):\n",
    "    \n",
    "        \"\"\"\n",
    "        Сборка челевых переменных (победа, тотал м/б, число выигранных раундов в 1/2 половинах за обе стороны)\n",
    "        \"\"\"    \n",
    "        df_targets = pd.DataFrame()\n",
    "        for d_rsp in tqdm.tqdm(self.L_COLLECTION):  \n",
    "\n",
    "            try:\n",
    "                \n",
    "                game_id = d_rsp['id']\n",
    "                if game_id in L_GAME_IDXS:\n",
    "                    ###########################################################################    \n",
    "                    df_rounds = pd.DataFrame.from_records(d_rsp['rounds'])\n",
    "\n",
    "                    maxround = df_rounds['round'].max()\n",
    "                    start_ct_id = df_rounds.query('round==1')['ct'].iloc[0]\n",
    "                    start_t_id = df_rounds.query('round==1')['terrorists'].iloc[0]\n",
    "                    df_h1 = df_rounds.query('round<=15')\n",
    "                    df_h2 = df_rounds.query('round>15')\n",
    "                    d_h1_win_count = df_h1['winner_team'].value_counts().to_dict()\n",
    "                    d_h2_win_count = df_h2['winner_team'].value_counts().to_dict()\n",
    "                    d_h1h2_win_count = df_rounds['winner_team'].value_counts().to_dict()\n",
    "                    winner_id = df_rounds['winner_team'].value_counts().idxmax()\n",
    "                    \n",
    "\n",
    "                    #############################################################################\n",
    "\n",
    "                    d_targets4game = {'id':game_id}\n",
    "                    \n",
    "                    d_targets4game['start_ct__win'] = int(winner_id==start_ct_id)\n",
    "\n",
    "                    for i in range(16, 31):\n",
    "\n",
    "                        d_targets4game[f'total__b__{i}'] = int(maxround>=i)\n",
    "                        d_targets4game[f'total__m__{i}'] = int(maxround<=i)\n",
    "\n",
    "                    for i in range(1, 16):\n",
    "\n",
    "                        d_targets4game[f'h1__start_ct_win__b__{i}'] = int(d_h1_win_count[start_ct_id]>=i)\n",
    "                        d_targets4game[f'h1__start_ct_win__m__{i}'] = int(d_h1_win_count[start_ct_id]<=i)    \n",
    "                        d_targets4game[f'h1__start_t_win__b__{i}'] = int(d_h1_win_count[start_t_id]>=i)\n",
    "                        d_targets4game[f'h1__start_t_win__m__{i}'] = int(d_h1_win_count[start_t_id]<=i)\n",
    "\n",
    "                        d_targets4game[f'h2__start_ct_win__b__{i}'] = int(d_h2_win_count[start_ct_id]>=i)\n",
    "                        d_targets4game[f'h2__start_ct_win__m__{i}'] = int(d_h2_win_count[start_ct_id]<=i)    \n",
    "                        d_targets4game[f'h2__start_t_win__b__{i}'] = int(d_h1_win_count[start_t_id]>=i)\n",
    "                        d_targets4game[f'h2__start_t_win__m__{i}'] = int(d_h1_win_count[start_t_id]<=i)\n",
    "\n",
    "                        d_targets4game[f'h1h2__start_ct_win__b__{i}'] = int(d_h1h2_win_count[start_ct_id]>=i)\n",
    "                        d_targets4game[f'h1h2__start_ct_win__m__{i}'] = int(d_h1h2_win_count[start_ct_id]<=i)\n",
    "                        d_targets4game[f'h1h2__start_t_win__b__{i}'] = int(d_h1h2_win_count[start_t_id]>=i)\n",
    "                        d_targets4game[f'h1h2__start_t_win__m__{i}'] = int(d_h1h2_win_count[start_t_id]<=i)                     \n",
    "\n",
    "                    df_targets = df_targets.append(d_targets4game, ignore_index = True)\n",
    "\n",
    "            except:\n",
    "                pass \n",
    "        df_targets['id'] = df_targets['id'].astype(int)\n",
    "        \n",
    "        return df_targets\n",
    "\n",
    "    def prepare_data(self, df_targets, df_features):\n",
    "\n",
    "        df_targets = df_targets.set_index('id').astype(int)\n",
    "        df_features = df_features.set_index('id')\n",
    "        games2use= np.intersect1d(df_features.index, df_targets.index)\n",
    "\n",
    "        X = df_features.loc[games2use]\n",
    "        del df_features\n",
    "        gc.collect()\n",
    "        L_CAT_FEATURES = [\n",
    "            'number_of_games', 'year', 'month', 'day', 'weekday', 'hour',\n",
    "            'map_id', 'league_id', 'serie_id', 'tournament_id', 'serie_tier',\n",
    "            'start_t__team_id', 'start_t__team_lineup', 'start_t__team_location',\n",
    "            'start_t__player1_id', 'start_t__player1_nationality',\n",
    "            'start_t__player1_birthday_year', 'start_t__player1_birthday_month',\n",
    "            'start_t__player1_birthday_day', 'start_t__player2_id',\n",
    "            'start_t__player2_nationality', 'start_t__player2_birthday_year',\n",
    "            'start_t__player2_birthday_month', 'start_t__player2_birthday_day',\n",
    "            'start_t__player3_id', 'start_t__player3_nationality',\n",
    "            'start_t__player3_birthday_year', 'start_t__player3_birthday_month',\n",
    "            'start_t__player3_birthday_day', 'start_t__player4_id',\n",
    "            'start_t__player4_nationality', 'start_t__player4_birthday_year',\n",
    "            'start_t__player4_birthday_month', 'start_t__player4_birthday_day',\n",
    "            'start_t__player5_id', 'start_t__player5_nationality',\n",
    "            'start_t__player5_birthday_year', 'start_t__player5_birthday_month',\n",
    "            'start_t__player5_birthday_day', 'start_ct__team_id',\n",
    "            'start_ct__team_lineup', 'start_ct__team_location',\n",
    "            'start_ct__player1_id', 'start_ct__player1_nationality',\n",
    "            'start_ct__player1_birthday_year', 'start_ct__player1_birthday_month',\n",
    "            'start_ct__player1_birthday_day', 'start_ct__player2_id',\n",
    "            'start_ct__player2_nationality', 'start_ct__player2_birthday_year',\n",
    "            'start_ct__player2_birthday_month', 'start_ct__player2_birthday_day',\n",
    "            'start_ct__player3_id', 'start_ct__player3_nationality',\n",
    "            'start_ct__player3_birthday_year', 'start_ct__player3_birthday_month',\n",
    "            'start_ct__player3_birthday_day', 'start_ct__player4_id',\n",
    "            'start_ct__player4_nationality', 'start_ct__player4_birthday_year',\n",
    "            'start_ct__player4_birthday_month', 'start_ct__player4_birthday_day',\n",
    "            'start_ct__player5_id', 'start_ct__player5_nationality',\n",
    "            'start_ct__player5_birthday_year', 'start_ct__player5_birthday_month',\n",
    "            'start_ct__player5_birthday_day'\n",
    "        ]\n",
    "\n",
    "        for key in L_CAT_FEATURES:\n",
    "            try:\n",
    "                X[key] = X[key].fillna(-9999).astype(int).astype('category')\n",
    "            except:\n",
    "                X[key] = X[key].fillna('default').astype('category')\n",
    "\n",
    "        L_NUM_FEATURES = X.drop(L_CAT_FEATURES, 1).columns\n",
    "        X[L_NUM_FEATURES] = X[L_NUM_FEATURES].fillna(-9999)\n",
    "        \n",
    "        Y = df_targets.loc[games2use]\n",
    "        del df_targets\n",
    "        gc.collect()    \n",
    "\n",
    "        # X_obj = X.select_dtypes('category').astype('object')\n",
    "        # L_obj_keys = X_obj.columns\n",
    "        # for cmb in itertools.combinations(L_obj_keys, 2):\n",
    "        #     cmb= list(cmb)\n",
    "        #     new_key = '-'.join([str(x) for x in cmb])    \n",
    "        #     X[new_key] = X_obj[cmb].astype('str').apply(lambda x: '-'.join(x), axis = 1).astype('category')\n",
    "        # del X_obj\n",
    "        # gc.collect()\n",
    "\n",
    "        return X, Y  \n",
    "\n",
    "    def update_dataset(self, X, Y, PATH_TO_DATASET):\n",
    "\n",
    "        try:\n",
    "            dataset = pd.read_pickle(PATH_TO_DATASET)   \n",
    "            i = dataset['iter'].max()\n",
    "            assert (X.index==Y.index).all()\n",
    "            new_dataset = pd.concat([X.add_prefix('FEATURE_'), Y.add_prefix('TARGET_')], 1)\n",
    "            del X, Y\n",
    "            gc.collect()            \n",
    "            new_dataset['update_at'] = datetime.now()\n",
    "            new_dataset['iter'] = i+1\n",
    "            dataset = pd.concat([dataset, new_dataset])\n",
    "            dataset.to_pickle(PATH_TO_DATASET)\n",
    "        except:\n",
    "            assert (X.index==Y.index).all()\n",
    "            new_dataset = pd.concat([X.add_prefix('FEATURE_'), Y.add_prefix('TARGET_')], 1)\n",
    "            del X, Y\n",
    "            gc.collect() \n",
    "            new_dataset['update_at'] = datetime.now()\n",
    "            new_dataset['iter'] = 1\n",
    "            new_dataset.to_pickle(PATH_TO_DATASET)  \n",
    "    def run_ml_pipelines(self, PATH_TO_DATASET):\n",
    "\n",
    "        def objective(trial):\n",
    "\n",
    "            param = {\n",
    "                \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\"]),\n",
    "                \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", .5, 1),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 11),\n",
    "                \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "                \"bootstrap_type\": trial.suggest_categorical(\n",
    "                    \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"]\n",
    "                )\n",
    "            }\n",
    "\n",
    "            if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "                param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "            elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "                param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "            param.update(CONST_PARAMS)\n",
    "            param['cat_features'] = np.where(X_train.dtypes=='category')[0]\n",
    "            param['use_best_model'] = True\n",
    "            params['random_state'] = SEED\n",
    "\n",
    "            model = cb.CatBoostClassifier(**param)\n",
    "            model.fit(X_train, y_train,  eval_set=(X_hold, y_hold), early_stopping_rounds=100)\n",
    "\n",
    "            score = roc_auc_score(y_hold, model.predict_proba(X_hold)[:, 1])    \n",
    "            \n",
    "            return score\n",
    "\n",
    "        CONST_PARAMS= {\n",
    "            'iterations':1000,\n",
    "            'loss_function':'Logloss',    \n",
    "            'verbose':0,\n",
    "            }\n",
    "        SEED=13\n",
    "        N_PERM_ITER = 20\n",
    "\n",
    "        dataset = pd.read_pickle(PATH_TO_DATASET)\n",
    "\n",
    "        X = dataset.loc[:, dataset.columns.str.contains('FEATURE')]\n",
    "        Y = dataset.loc[:, dataset.columns.str.contains('TARGET')]\n",
    "        del dataset\n",
    "        gc.collect()\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .1, shuffle = False)\n",
    "        X_train, X_hold, Y_train, Y_hold = train_test_split(X_train, Y_train, test_size = .2, shuffle = False)\n",
    "        del X, Y\n",
    "        gc.collect()\n",
    "\n",
    "        L_ALL_TARGETS = Y_train.columns\n",
    "        D_PPL_RESULTS = {}\n",
    "        for i, key in enumerate(L_ALL_TARGETS):\n",
    "            \n",
    "            try:\n",
    "\n",
    "                print('> target#{}/{}: {}'.format(i+1, len(L_ALL_TARGETS), key))\n",
    "                \n",
    "                y_train = Y_train[key].astype(int)\n",
    "                y_hold = Y_hold[key].astype(int)\n",
    "                y_test = Y_test[key].astype(int)\n",
    "\n",
    "                print('----------------------------------------------------------------------------------\\n')\n",
    "\n",
    "                print('> subspace feature selection ...')\n",
    "                i = 0\n",
    "                while True:    \n",
    "\n",
    "                    i+=1\n",
    "                    n_features = X_train.shape[1]\n",
    "                    print('\\t> iter#{}. n_features: {}'.format(i, n_features))\n",
    "\n",
    "                    n_games = X_train.shape[0]\n",
    "                    batch_size = 5000        \n",
    "                    n_batches = np.int32(np.ceil(n_features/batch_size))\n",
    "                    L_feature_batch = np.array_split(X_train.columns, n_batches)\n",
    "\n",
    "                    if len(L_feature_batch)==1:\n",
    "                        break\n",
    "                    else:\n",
    "                        L_feat2use = []\n",
    "                        for batch in tqdm.tqdm(L_feature_batch):\n",
    "\n",
    "                            try:\n",
    "                                X_batch_train, X_batch_hold = X_train[batch], X_hold[batch]\n",
    "\n",
    "                                params = CONST_PARAMS.copy()\n",
    "                                params['cat_features'] = np.where(X_batch_train.dtypes=='category')[0]\n",
    "                                params['use_best_model'] = True\n",
    "                                params['random_state'] = SEED\n",
    "\n",
    "                                model = cb.CatBoostClassifier(**params)   \n",
    "                                model.fit(X_batch_train, y_train,  eval_set=(X_batch_hold, y_hold), early_stopping_rounds=100)\n",
    "\n",
    "                                mask = model.feature_importances_>0            \n",
    "                                L_feat2use.extend(X_batch_train.columns[mask].tolist())\n",
    "                                del X_batch_train, X_batch_hold\n",
    "                                gc.collect()  \n",
    "\n",
    "                            except:\n",
    "                                pass \n",
    "\n",
    "                        X_train_c, X_hold_c = X_train[L_feat2use], X_hold[L_feat2use]\n",
    "                        del X_train, X_hold\n",
    "                        X_train, X_hold = X_train_c, X_hold_c\n",
    "                        del X_train_c, X_hold_c\n",
    "                        gc.collect()\n",
    "                print('----------------------------------------------------------------------------------\\n')   \n",
    "\n",
    "                print('> recursive feature selection ...')\n",
    "                i=0\n",
    "                while True:\n",
    "                    \n",
    "                    i+=1\n",
    "                    n_features = X_train.shape[1]\n",
    "                    print('\\t> iter#{}. n_features: {}'.format(i, n_features))\n",
    "\n",
    "                    params = CONST_PARAMS.copy()\n",
    "                    params['cat_features'] = np.where(X_train.dtypes=='category')[0]\n",
    "                    params['use_best_model'] = True\n",
    "                    params['random_state'] = SEED\n",
    "\n",
    "                    model = cb.CatBoostClassifier(**params)   \n",
    "                    model.fit(X_train, y_train,  eval_set=(X_hold, y_hold), early_stopping_rounds=100)\n",
    "                    \n",
    "                    mask = model.feature_importances_>0     \n",
    "                    if np.all(mask):\n",
    "                        break\n",
    "                    else:\n",
    "                        X_train_c, X_hold_c = X_train.loc[:, mask], X_hold.loc[:, mask]\n",
    "                        del X_train, X_hold\n",
    "                        X_train, X_hold = X_train_c, X_hold_c\n",
    "                        del X_train_c, X_hold_c\n",
    "                        gc.collect()\n",
    "                print('----------------------------------------------------------------------------------\\n')   \n",
    "\n",
    "                print('> recursive feature selection with permutation importances...')\n",
    "                i=0\n",
    "                while True:\n",
    "\n",
    "                    i+=1\n",
    "                    n_features = X_train.shape[1]\n",
    "                    print('\\t> iter#{}. n_features: {}'.format(i, n_features))\n",
    "\n",
    "                    study = optuna.create_study(\n",
    "                        sampler=TPESampler(),    \n",
    "                        direction=\"maximize\"\n",
    "                        )\n",
    "                    study.optimize(objective, n_trials=100, timeout=60*10)\n",
    "\n",
    "                    best_params = {}\n",
    "                    best_params.update(CONST_PARAMS)\n",
    "                    best_params['cat_features'] = np.where(X_train.dtypes=='category')[0]\n",
    "                    best_params['use_best_model'] = True\n",
    "                    best_params['random_state'] = SEED\n",
    "                    best_params.update(study.best_trial.params)\n",
    "\n",
    "                    model = cb.CatBoostClassifier(**best_params)\n",
    "                    model.fit(X_train, y_train,  eval_set=(X_hold, y_hold), early_stopping_rounds=100)\n",
    "                    ho_score_before = roc_auc_score(y_hold, model.predict_proba(X_hold)[:, 1])\n",
    "\n",
    "                    z_perm_imp = np.zeros((X_train.shape[1], ))\n",
    "                    for _ in tqdm.tqdm(range(N_PERM_ITER)):\n",
    "\n",
    "                        model.fit(X_train, y_train,  eval_set=(X_hold, y_hold), early_stopping_rounds=100)\n",
    "                        \n",
    "                        imp = permutation_importance(\n",
    "                                model,\n",
    "                                X_hold, y_hold,\n",
    "                                scoring = 'roc_auc',\n",
    "                                n_repeats=1,\n",
    "                                random_state = SEED+_,\n",
    "                                n_jobs=-1\n",
    "                                )['importances_mean'].flatten()\n",
    "                                \n",
    "                        z_perm_imp += imp/N_PERM_ITER\n",
    "\n",
    "                    mask = z_perm_imp>0\n",
    "                    best_params['cat_features'] = np.where(X_train.loc[:, mask].dtypes=='category')[0]\n",
    "                    model = cb.CatBoostClassifier(**best_params)\n",
    "                    model.fit(X_train.loc[:, mask], y_train,  eval_set=(X_hold.loc[:, mask], y_hold), early_stopping_rounds=100)\n",
    "                    ho_score_after = roc_auc_score(y_hold, model.predict_proba(X_hold.loc[:, mask])[:, 1])\n",
    "\n",
    "                    if ho_score_after>ho_score_before:\n",
    "                        X_train, X_hold = X_train.loc[:, mask], X_hold.loc[:, mask]\n",
    "                    else:\n",
    "                        X_test = X_test[X_train.columns]        \n",
    "                        best_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "                        best_features = X_test.columns\n",
    "                print('----------------------------------------------------------------------------------\\n')  \n",
    "\n",
    "                del X_train, X_hold, X_test, y_train, y_hold, y_test\n",
    "                gc.collect()\n",
    "\n",
    "                d_res = {'score':best_score, 'features':best_features, 'params':best_params}\n",
    "                D_PPL_RESULTS[key] = d_res\n",
    "                del d_res\n",
    "                gc.collect()\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return D_PPL_RESULTS\n",
    "\n",
    "    def fit(self, PATH_TO_RESPONSES, PATH_TO_FEATURES_GAMEINFO, PATH_TO_FEATURES_TEAM, PATH_TO_FEATURES_PLAYER, PATH_TO_DATASET):\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> collecting responses ...')\n",
    "        # # коллекция респонсов\n",
    "        # self.L_COLLECTION = self.get_game_collection(PATH_TO_RESPONSES)\n",
    "        # print('----------------------------------------------------------------------------------\\n')\n",
    "\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> preparing team/player profiles ...')\n",
    "        # # профайлинг игроков и команд в играх\n",
    "        # self.df_player_profile, self.df_team_profile = self.get_profiles(self.L_COLLECTION)        \n",
    "        # gc.collect()\n",
    "        # print('----------------------------------------------------------------------------------\\n')\n",
    "\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> collecting features: 1. game info ...')\n",
    "        # self.add_features__gameinfo(PATH_TO_FEATURES_GAMEINFO)\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> collecting features: 2. team history aggregation ...')\n",
    "        # self.add_features__team(PATH_TO_FEATURES_TEAM)\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> collecting features: 3. player history aggregation ...')\n",
    "        # self.add_features__player(PATH_TO_FEATURES_PLAYER)\n",
    "        # print('----------------------------------------------------------------------------------\\n')\n",
    "\n",
    "        # del self.df_player_profile, self.df_team_profile\n",
    "        # gc.collect()\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> building features ...')\n",
    "        # df_features = self.build_features(PATH_TO_FEATURES_GAMEINFO, PATH_TO_FEATURES_TEAM, PATH_TO_FEATURES_PLAYER)\n",
    "        # df_features.to_pickle('df_features.pickle')\n",
    "        # df_features_c = pd.read_pickle('df_features.pickle')\n",
    "        # df_features = df_features_c.iloc[-5000:]\n",
    "        # del df_features_c\n",
    "        # gc.collect()\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> building targets ...')\n",
    "        # L_GAME_IDXS = np.unique(df_features['id'])\n",
    "        # df_targets = self.build_targets(L_GAME_IDXS) \n",
    "        # df_targets.to_pickle('df_targets.pickle')\n",
    "        # df_targets = pd.read_pickle('df_targets.pickle')\n",
    "        # print('----------------------------------------------------------------------------------\\n')\n",
    "\n",
    "        # time.sleep(1)\n",
    "        # print('> preparing dataset for ml ...')\n",
    "        # X, Y = self.prepare_data(df_targets, df_features)\n",
    "        # del df_targets, df_features\n",
    "        # gc.collect()\n",
    "        # X.to_pickle('X.pickle'), Y.to_pickle('Y.pickle')\n",
    "        # print('----------------------------------------------------------------------------------\\n')   \n",
    "\n",
    "        \n",
    "        # time.sleep(1)\n",
    "        # print('> updating dataset version  ...')\n",
    "        # self.update_dataset(X, Y, PATH_TO_DATASET)\n",
    "        # print('----------------------------------------------------------------------------------\\n')   \n",
    "\n",
    "        time.sleep(1)\n",
    "        print('> running ml pipelines  ...')\n",
    "        self.d_run_result = self.run_ml_pipelines(PATH_TO_DATASET)\n",
    "        with open('d_run_result.pickle', 'wb') as f:\n",
    "            pickle.dump(self.d_run_result, f)\n",
    "        print('----------------------------------------------------------------------------------\\n')   \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> running ml pipelines  ...\n"
     ]
    }
   ],
   "source": [
    "# директория с коллекцией респонсов\n",
    "PATH_TO_RESPONSES = 'L_games_collection'\n",
    "# директория с коллекцией признаков для игр (информация об игре)\n",
    "PATH_TO_FEATURES_GAMEINFO = r'D:\\\\features_gameinfo'\n",
    "# директория с коллекцией признаков для игр (командная статистика)\n",
    "PATH_TO_FEATURES_TEAM = r'D:\\\\features_team'\n",
    "# директория с коллекцией признаков для игр (статистика игроков)\n",
    "PATH_TO_FEATURES_PLAYER = r'D:\\\\features_player'\n",
    "# датасет\n",
    "PATH_TO_DATASET = 'dataset.pickle'\n",
    "\n",
    "# модель\n",
    "csgo_ml = CsgoOutcomePredictor()\n",
    "csgo_ml.fit(\n",
    "    PATH_TO_RESPONSES,\n",
    "    PATH_TO_FEATURES_GAMEINFO,\n",
    "    PATH_TO_FEATURES_TEAM,\n",
    "    PATH_TO_FEATURES_PLAYER,\n",
    "    PATH_TO_DATASET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc9092704b76bcd3825fbb7a05cb662b50152522ace34f313990aca7aedee1c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
