{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm, json, pickle, gc, zipfile, itertools, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, ParameterGrid, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tqdm.contrib.concurrent import process_map  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap \n",
    "from sklearn.model_selection import KFold\n",
    "from nancorrmp.nancorrmp import NaNCorrMp\n",
    "\n",
    "def run_ml_pipeline(X, y):\n",
    "\n",
    "    # сплит\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size = TEST_SIZE, shuffle = False)\n",
    "    X_tr, X_ho, y_tr, y_ho = train_test_split(X_tr, y_tr, test_size = HOLD_SIZE, shuffle = False)\n",
    "\n",
    "    # бъем признаки на батчи, чтобы не перегрузить оперативку\n",
    "    L_all_keys = X_tr.columns\n",
    "    L_batches = np.array_split(L_all_keys, np.int32(np.ceil(len(L_all_keys)/25000)))\n",
    "    \n",
    "    # отбираем признаки с ненулевой важностью\n",
    "    L_feat2use = []\n",
    "    for i, batch in enumerate(L_batches):\n",
    "        print('> batch#{}/{}'.format(i+1, len(L_batches)))\n",
    "        x_tr_batch = X_tr[batch]\n",
    "        x_ho_batch = X_ho[batch]\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(x_tr_batch.dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params)        \n",
    "        model.fit(x_tr_batch, y_tr, eval_set=(x_ho_batch, y_ho), early_stopping_rounds=50)\n",
    "        mask = model.feature_importances_>0\n",
    "        L_feat2use.extend(batch[mask].tolist())\n",
    "        del x_tr_batch, x_ho_batch\n",
    "    X_tr_c, X_ho_c, X_te_c = X_tr[L_feat2use], X_ho[L_feat2use], X_te[L_feat2use]\n",
    "    del X_tr, X_ho, X_te\n",
    "    X_tr, X_ho, X_te = X_tr_c, X_ho_c, X_te_c\n",
    "    del X_tr_c, X_ho_c, X_te_c\n",
    "    gc.collect()\n",
    "\n",
    "    # рекурсивный отбор с ранней остановкой\n",
    "    i = 1\n",
    "    while True:\n",
    "        print('> iter#{}'.format(i))\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params)        \n",
    "        model.fit(X_tr, y_tr, eval_set=(X_ho, y_ho), early_stopping_rounds=50)\n",
    "        mask = model.feature_importances_>0\n",
    "        if np.all(mask):\n",
    "            break\n",
    "        else:\n",
    "            X_tr, X_ho = X_tr.loc[:, mask], X_ho.loc[:, mask]\n",
    "            i+=1\n",
    "    X_te = X_te[X_tr.columns]\n",
    "    # оптимизация гиперпараметров\n",
    "    params = CONST_PARAMS.copy()\n",
    "    params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "    params['iterations'] = model.best_iteration_\n",
    "    params['verbose'] =0\n",
    "    model = cb.CatBoostClassifier(**params)    \n",
    "\n",
    "    cb_opt = CatBoostOptimizer(\n",
    "                    scoring_func= lambda y, y_proba: roc_auc_score(y, y_proba),\n",
    "                    const_params=params,\n",
    "                    seed=SEED, \n",
    "                    direction='maximize',\n",
    "                    n_trials=15\n",
    "        )\n",
    "\n",
    "    cb_opt.fit(X_tr, y_tr)\n",
    "    best_params = cb_opt.transform()\n",
    "    best_params['verbose'] = 0\n",
    "    best_params['random_state'] = SEED\n",
    "    \n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "\n",
    "        print('> permutation importance iter#{}. n_features = {}'.format(i, X_tr.shape[1]))\n",
    "\n",
    "        params = best_params.copy()        \n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        params['iterations'] = model.best_iteration_\n",
    "        te_score_before=roc_auc_score(y_te, model.predict_proba(X_te)[:, 1])\n",
    "\n",
    "        L_perm_imp = []\n",
    "        for j in tqdm.tqdm(range(100)):\n",
    "            d_perm_imp = permutation_importance(model, X_ho, y_ho, scoring='roc_auc', n_repeats=1, random_state = SEED+j, n_jobs=-1)\n",
    "            L_perm_imp.append(d_perm_imp['importances_mean'].flatten())\n",
    "        arr_perm_imp_mean = np.mean(np.r_[L_perm_imp], 0)\n",
    "        idx_selected = np.where(arr_perm_imp_mean>0)[0]\n",
    "\n",
    "        params_c=params.copy()\n",
    "        params_c['cat_features'] = np.where(X_tr.iloc[:, idx_selected].dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params_c)\n",
    "        model.fit(X_tr.iloc[:, idx_selected], y_tr)\n",
    "        params_c['iterations'] = model.best_iteration_\n",
    "        te_score_after = roc_auc_score(y_te, model.predict_proba(X_te.iloc[:, idx_selected])[:, 1])\n",
    "\n",
    "        print('\\t> score before: {:.2f}, score after: {:.2f}'.format(te_score_before, te_score_after))\n",
    "        if te_score_after > te_score_before:\n",
    "            best_score = te_score_after\n",
    "            X_tr, X_ho, X_te = X_tr.iloc[:, idx_selected], X_ho.iloc[:, idx_selected], X_te.iloc[:, idx_selected]\n",
    "            i+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # оптимизация гиперпараметров\n",
    "    params = CONST_PARAMS.copy()\n",
    "    params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "    params['iterations'] = model.best_iteration_\n",
    "    params['verbose'] =0\n",
    "    model = cb.CatBoostClassifier(**params)    \n",
    "\n",
    "    cb_opt = CatBoostOptimizer(\n",
    "                    scoring_func= lambda y, y_proba: roc_auc_score(y, y_proba),\n",
    "                    const_params=params,\n",
    "                    seed=SEED, \n",
    "                    direction='maximize',\n",
    "                    n_trials=30\n",
    "        )\n",
    "    cb_opt.fit(X_tr, y_tr)\n",
    "    best_params = cb_opt.transform()\n",
    "    best_params['verbose'] = 0   \n",
    "    best_params['cat_features'] = np.where(X_tr.iloc[:, idx_selected].dtypes=='category')[0]\n",
    "    best_params['random_state'] = SEED \n",
    "    \n",
    "    features = X_tr.columns\n",
    "    d_res = {'params':best_params, 'roc_auc':best_score, 'features':features}\n",
    "\n",
    "    X_tr, X_ho, X_te, y_tr, y_ho, y_te\n",
    "    gc.collect()\n",
    "\n",
    "    return d_res\n",
    "\n",
    "def get_game_collection(PATH_TO_DIR):\n",
    "    \n",
    "    \"\"\"\n",
    "    Описание: коллекционирование респонсов парсера\n",
    "    Параметры: PATH_TO_DIR - путь до директории с респонсами\n",
    "    \"\"\"\n",
    "\n",
    "    L_FILENAMES = os.listdir(PATH_TO_DIR)\n",
    "    L_COLLECTION = []\n",
    "    for fnm in tqdm.tqdm(L_FILENAMES):\n",
    "        try:\n",
    "            pth = os.path.join(PATH_TO_DIR, fnm)\n",
    "            with open(pth, 'r') as f:\n",
    "                d_rsp = json.load(f)\n",
    "            L_COLLECTION.append(d_rsp)\n",
    "        except:\n",
    "            pass\n",
    "    idx_ordered = np.argsort([d_game['id'] for d_game in L_COLLECTION])[::-1]\n",
    "    L_COLLECTION = np.array(L_COLLECTION)[idx_ordered].tolist()\n",
    "    return L_COLLECTION\n",
    "\n",
    "def get_profiles(L_COLLECTION):\n",
    "\n",
    "    \"\"\"\n",
    "    Описание: профайлинг игроков в играх\n",
    "    Параметры: L_COLLECTION- коллекция респонсов\n",
    "    \"\"\"\n",
    "\n",
    "    def add_profile(d_rsp):\n",
    "\n",
    "        def add_global_info(d_game):\n",
    "\n",
    "            d = {}\n",
    "\n",
    "            d['id'] = d_game['id']\n",
    "            d['match_id'] = d_game['match_id']\n",
    "            d['match_type'] = d_game['match']['match_type']\n",
    "            d['number_of_games'] = d_game['match']['number_of_games']\n",
    "            d['date'] = parser.parse(d_game['begin_at'])\n",
    "            d['map_id'] = d_game['map']['id']\n",
    "            d['league_id'] = d_game['match']['league']['id']\n",
    "            d['serie_id'] = d_game['match']['serie']['id']\n",
    "            d['tournament_id'] = d_game['match']['tournament']['id']\n",
    "            d['serie_tier'] = d_game['match']['serie']['tier']\n",
    "\n",
    "            return d\n",
    "        \n",
    "        # идентификаторы актуальных карт\n",
    "        l_map2use = [1, 2, 6, 7, 8, 20, 31]\n",
    "        # ключи со статистикой игрока\n",
    "        l_stat_keys = ['adr', 'assists', 'deaths', 'first_kills_diff', 'flash_assists', \n",
    "                       'headshots', 'k_d_diff', 'kast', 'kills', 'rating']\n",
    "\n",
    "        # информация об игре\n",
    "        d_info = add_global_info(d_game)\n",
    "        \n",
    "        if d_info['map_id'] in l_map2use:  \n",
    "\n",
    "            d_r1 = d_rsp['rounds'][0]\n",
    "            if d_r1['round']==1:\n",
    "                \n",
    "                # информация о раундах\n",
    "                df_rounds = pd.DataFrame.from_records(d_rsp['rounds'])\n",
    "                start_ct_id =d_r1['ct']   \n",
    "                winner_id = df_rounds['winner_team'].value_counts().idxmax()\n",
    "                maxround = df_rounds['round'].max()\n",
    "                d_h1_win_count = df_rounds.query('round<=15')['winner_team'].value_counts().to_dict()\n",
    "                d_h2_win_count = df_rounds.query('round>15')['winner_team'].value_counts().to_dict()\n",
    "                d_h1_outcome_count = df_rounds.query('round<=15')['outcome'].value_counts().to_dict()\n",
    "                d_h2_outcome_count = df_rounds.query('round>15')['outcome'].value_counts().to_dict()        \n",
    "\n",
    "                L = []\n",
    "                counter = 0\n",
    "                # информация об игроках\n",
    "                for p in d_rsp['players']:\n",
    "                    counter+=1\n",
    "\n",
    "                    d = {}\n",
    "                    d.update(d_info)\n",
    "\n",
    "                    # идентификатор игрока\n",
    "                    d['player_id'] = p['player']['id']\n",
    "                    # идентификатор команды\n",
    "                    d['team_id'] = p['team']['id']\n",
    "                    # идентификатор оппонента\n",
    "                    d['opponent_id'] = p['opponent']['id']\n",
    "\n",
    "                    # национальность игрока\n",
    "                    d['player_nationality']  = p['player']['nationality']\n",
    "                    # дата рождения игрока\n",
    "                    d['player_birthday']  = p['player']['birthday']\n",
    "                    # страна команды\n",
    "                    d['team_location']  = p['team']['location']\n",
    "\n",
    "                    # сторона начала\n",
    "                    d['start_ct']= 1 if start_ct_id==d['team_id'] else 0\n",
    "                    # победа\n",
    "                    d['win'] = 1 if winner_id==d['team_id'] else 0\n",
    "                    # все раундов в игре\n",
    "                    d['maxround'] = maxround\n",
    "\n",
    "                    # число выигранных раундов в 1-ой половине игры\n",
    "                    try:\n",
    "                        d['h1_win_count'] = d_h1_win_count[d['team_id']]\n",
    "                    except:\n",
    "                        d['h1_win_count'] = 0 \n",
    "                    # число выигранных раундов во 2-ой половине игры\n",
    "                    try:\n",
    "                        d['h2_win_count'] = d_h2_win_count[d['team_id']]\n",
    "                    except:\n",
    "                        d['h2_win_count'] = 0 \n",
    "                    # исходы раундов в 1-ой половине игры\n",
    "                    for k, v in d_h1_outcome_count.items():\n",
    "                        d[f'h1_outcome_{k}_count'] = v\n",
    "                    # исходы раундов во 2-ой половине игры\n",
    "                    for k, v in d_h2_outcome_count.items():\n",
    "                        d[f'h2_outcome_{k}_count'] = v            \n",
    "\n",
    "                    # статистика игрока\n",
    "                    d.update({k:p[k] if pd.notnull(p[k]) else 0.0 for k in l_stat_keys})\n",
    "\n",
    "                    L.append(d)\n",
    "                if counter==10:\n",
    "                    return L\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "    # информация об игре\n",
    "    L_GLOBAL_KEYS = [\n",
    "        'id', 'match_id', 'match_type', 'number_of_games',\n",
    "        'date', 'year', 'month', 'day', 'weekday', 'hour',\n",
    "        'map_id',\n",
    "        'league_id', 'serie_id', 'tournament_id', 'serie_tier',\n",
    "        'start_ct'\n",
    "    ]\n",
    "\n",
    "    # ключи для агрегирования\n",
    "    L_AGG_KEYS = [    \n",
    "        \n",
    "        'h1_outcome_defused_count', 'h1_outcome_eliminated_count',\n",
    "        'h1_outcome_exploded_count', 'h1_outcome_timeout_count',\n",
    "        'h1_win_count', 'h2_outcome_defused_count',\n",
    "        'h2_outcome_eliminated_count', 'h2_outcome_exploded_count',\n",
    "        'h2_outcome_timeout_count', 'h2_win_count',\n",
    "\n",
    "        'adr', 'assists', 'deaths', 'first_kills_diff', 'flash_assists', 'headshots',\n",
    "        'k_d_diff', 'kast', 'kills', 'maxround', 'rating', 'win'\n",
    "    ]\n",
    "\n",
    "    # ключи для группировки\n",
    "    L_GROUP_KEYS = [\n",
    "        'team_id', 'opponent_id', 'team_location', 'lineup'\n",
    "    ]\n",
    "    # профайлинг игрока\n",
    "    L_player_profile = []\n",
    "    for d_game in tqdm.tqdm(L_COLLECTION):\n",
    "        try:\n",
    "            L_player_profile.extend(add_profile(d_game))        \n",
    "        except:\n",
    "            pass\n",
    "    df_player_profile = pd.DataFrame.from_records(L_player_profile)\n",
    "    del L_player_profile\n",
    "    gc.collect()\n",
    "\n",
    "    L_dict = []\n",
    "    for (game_id, team_id), subdf in tqdm.tqdm(df_player_profile.groupby(['id', 'team_id'])):\n",
    "        n_players = subdf.shape[0]\n",
    "        if n_players==5:\n",
    "            subdf_c = subdf.copy()\n",
    "            lineup = '-'.join(subdf['player_id'].sort_values().astype(str))\n",
    "            subdf_c['lineup'] = lineup\n",
    "            L_dict.extend(subdf_c.to_dict('records'))\n",
    "    del df_player_profile\n",
    "    gc.collect()\n",
    "    df_player_profile = pd.DataFrame.from_records(L_dict).sort_values('date')\n",
    "    del L_dict\n",
    "    gc.collect()\n",
    "\n",
    "    date = df_player_profile['date']\n",
    "    df_player_profile['year'] = date.dt.year\n",
    "    df_player_profile['month'] = date.dt.month\n",
    "    df_player_profile['day'] = date.dt.day\n",
    "    df_player_profile['weekday'] = date.dt.weekday\n",
    "    df_player_profile['hour'] = date.dt.hour\n",
    "    df_player_profile[['serie_tier', 'team_location']] = df_player_profile[['serie_tier', 'team_location']].fillna('default')    \n",
    "\n",
    "    # профайлинг команды\n",
    "    L_team_profile = []\n",
    "    for (game_id, team_id), subdf in tqdm.tqdm(df_player_profile.groupby(['id', 'team_id'])):    \n",
    "        d = subdf[L_GLOBAL_KEYS+L_GROUP_KEYS].iloc[0].to_dict()    \n",
    "        d.update(subdf[L_AGG_KEYS].mean().to_dict())\n",
    "        L_team_profile.append(d)\n",
    "    df_team_profile = pd.DataFrame.from_records(L_team_profile)\n",
    "    del L_team_profile\n",
    "    gc.collect()\n",
    "\n",
    "    return {'player':df_player_profile, 'team':df_team_profile}\n",
    "\n",
    "class CatBoostOptimizer():\n",
    "    \n",
    "    def __init__(self, scoring_func, const_params, seed, direction, n_trials):\n",
    "        self.scoring_func = scoring_func        \n",
    "        self.const_params = const_params\n",
    "        self.seed = seed\n",
    "        self.direction = direction\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def objective(self, trial):\n",
    "                \n",
    "        params = {\n",
    "#         'iterations':trial.suggest_int('iterations', 20, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.025, 0.25),\n",
    "        'depth': trial.suggest_int('depth', 3, 12),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 31),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "                }\n",
    "        \n",
    "        params.update(self.const_params)\n",
    "                \n",
    "        model = cb.CatBoostClassifier(**params, random_seed=self.seed)\n",
    "        model.fit(self.X_tr_c, self.y_tr_c, verbose=0, eval_set=(self.X_ho_c, self.y_ho_c))\n",
    "        \n",
    "        y_proba = model.predict_proba(self.X_ho_c)[:, 1]\n",
    "        \n",
    "        return self.scoring_func(self.y_ho_c, y_proba)\n",
    "    \n",
    "    def fit(self, X_tr, y_tr):\n",
    "        \n",
    "        self.cat_features = np.argwhere(X_tr.dtypes.values=='object').flatten()\n",
    "        \n",
    "        self.X_tr_c, self.X_ho_c, self.y_tr_c, self.y_ho_c = \\\n",
    "            train_test_split(X_tr, y_tr,\n",
    "                             test_size = .1,\n",
    "                             shuffle = True,\n",
    "                             random_state =self.seed)\n",
    "        \n",
    "        sampler = TPESampler(seed=self.seed)\n",
    "        study = optuna.create_study(direction=self.direction, sampler=sampler)\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        self.best_params = study.best_params\n",
    "        \n",
    "        del self.X_tr_c, self.X_ho_c, self.y_tr_c, self.y_ho_c\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self):\n",
    "        return self.best_params  \n",
    "\n",
    "def get_targets(L_COLLECTION, L_GAME_IDXS):\n",
    "    \n",
    "    \"\"\"\n",
    "    Извлечение челевых переменных (победа, тотал м/б, число выигранных раундов в 1/2 половинах за обе стороны)\n",
    "    \"\"\"    \n",
    "    df_targets = pd.DataFrame()\n",
    "    for d_rsp in tqdm.tqdm(L_COLLECTION):  \n",
    "\n",
    "        try:\n",
    "            \n",
    "            game_id = d_rsp['id']\n",
    "            if game_id in L_GAME_IDXS:\n",
    "                ###########################################################################    \n",
    "                df_rounds = pd.DataFrame.from_records(d_rsp['rounds'])\n",
    "\n",
    "                maxround = df_rounds['round'].max()\n",
    "                start_ct_id = df_rounds.query('round==1')['ct'].iloc[0]\n",
    "                start_t_id = df_rounds.query('round==1')['terrorists'].iloc[0]\n",
    "                df_h1 = df_rounds.query('round<=15')\n",
    "                df_h2 = df_rounds.query('round>15')\n",
    "                d_h1_win_count = df_h1['winner_team'].value_counts().to_dict()\n",
    "                d_h2_win_count = df_h2['winner_team'].value_counts().to_dict()\n",
    "                d_h1h2_win_count = df_rounds['winner_team'].value_counts().to_dict()\n",
    "                winner_id = df_rounds['winner_team'].value_counts().idxmax()\n",
    "                \n",
    "\n",
    "                #############################################################################\n",
    "\n",
    "                d_targets4game = {'id':game_id}\n",
    "                \n",
    "                d_targets4game['start_ct__win'] = int(winner_id==start_ct_id)\n",
    "\n",
    "                for i in range(16, 31):\n",
    "\n",
    "                    d_targets4game[f'total__b__{i}'] = int(maxround>=i)\n",
    "                    d_targets4game[f'total__m__{i}'] = int(maxround<=i)\n",
    "\n",
    "                for i in range(1, 16):\n",
    "\n",
    "                    d_targets4game[f'h1__start_ct_win__b__{i}'] = int(d_h1_win_count[start_ct_id]>=i)\n",
    "                    d_targets4game[f'h1__start_ct_win__m__{i}'] = int(d_h1_win_count[start_ct_id]<=i)    \n",
    "                    d_targets4game[f'h1__start_t_win__b__{i}'] = int(d_h1_win_count[start_t_id]>=i)\n",
    "                    d_targets4game[f'h1__start_t_win__m__{i}'] = int(d_h1_win_count[start_t_id]<=i)\n",
    "\n",
    "                    d_targets4game[f'h2__start_ct_win__b__{i}'] = int(d_h2_win_count[start_ct_id]>=i)\n",
    "                    d_targets4game[f'h2__start_ct_win__m__{i}'] = int(d_h2_win_count[start_ct_id]<=i)    \n",
    "                    d_targets4game[f'h2__start_t_win__b__{i}'] = int(d_h1_win_count[start_t_id]>=i)\n",
    "                    d_targets4game[f'h2__start_t_win__m__{i}'] = int(d_h1_win_count[start_t_id]<=i)\n",
    "\n",
    "                    d_targets4game[f'h1h2__start_ct_win__b__{i}'] = int(d_h1h2_win_count[start_ct_id]>=i)\n",
    "                    d_targets4game[f'h1h2__start_ct_win__m__{i}'] = int(d_h1h2_win_count[start_ct_id]<=i)\n",
    "                    d_targets4game[f'h1h2__start_t_win__b__{i}'] = int(d_h1h2_win_count[start_t_id]>=i)\n",
    "                    d_targets4game[f'h1h2__start_t_win__m__{i}'] = int(d_h1h2_win_count[start_t_id]<=i) \n",
    "\n",
    "                df_targets = df_targets.append(d_targets4game, ignore_index = True)\n",
    "\n",
    "        except:\n",
    "            pass \n",
    "    df_targets['id'] = df_targets['id'].astype(int)\n",
    "        \n",
    "    return df_targets\n",
    "\n",
    "def get_features(df_player_profile, L_GAME_IDXS):\n",
    "\n",
    "    \"\"\"\n",
    "    Описание: извлекает признаки для игры на основе профайлинга истории игр команд/игроков\n",
    "    Параметры: game_id - идентификатор игры\n",
    "    На выходе: словарь с признаками\n",
    "               ключи признаков формируются в соответствии со схемой:\n",
    "                    префикс: {сторона начала в текущей игре}__{признак команды/признак игрока}__{фильтр+группировка}___{тип агрегирования} \n",
    "                    типы фильтров: \n",
    "                        1. вся история игры команды\n",
    "                        2. итория игр команды в лиге, серии, турнире, карте и тд из текущей игры\n",
    "                        2. история игр команды на карте за сторону начала\n",
    "                        3. история игр команды с составом из текущей игры\n",
    "                        4. история игр пары команд\n",
    "                        5. история игр игрока в команде\n",
    "                        6. история игр игрока вне команды\n",
    "                    типы группировок:\n",
    "                        1. год, месяц, день, день недели, час\n",
    "                        2. тир серии\n",
    "                        3. число игр в матче (1, 3, 5)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def add_features(df_history, L_by_key, d_filter, L_agg_key, prefix):  \n",
    "        if d_filter is None:      \n",
    "            if L_by_key is None:    \n",
    "                d = {}\n",
    "                for agg_key in L_agg_key:\n",
    "                    ser = df_history[agg_key]\n",
    "                    d.update({f'{prefix}__{agg_key}__mean' : ser.mean(),\n",
    "                                f'{prefix}__{agg_key}__sum' : ser.sum()})\n",
    "                return d\n",
    "            else:\n",
    "                d = {}\n",
    "                for by_key in L_by_key:\n",
    "                    for by_value, subdf in df_history.groupby(by_key):\n",
    "                        for agg_key in L_agg_key:\n",
    "                            ser = subdf[agg_key]\n",
    "                            d.update({f'{prefix}__{by_key}_{by_value}__{agg_key}__mean' : ser.mean(),\n",
    "                                        f'{prefix}__{by_key}_{by_value}__{agg_key}__sum' : ser.sum()})\n",
    "                return d\n",
    "        else:\n",
    "            if L_by_key is None:    \n",
    "                d = {}            \n",
    "                for f_k, f_v in d_filter.items():\n",
    "                    df_hist=df_history[df_history[f_k]==f_v]\n",
    "                    for agg_key in L_agg_key:  \n",
    "                        ser = df_hist[agg_key]\n",
    "                        d.update({f'{prefix}__filter_{f_k}__{agg_key}__mean' : ser.mean(),\n",
    "                                    f'{prefix}__filter_{f_k}__{agg_key}__sum' : ser.sum()})\n",
    "                return d\n",
    "            else:\n",
    "                d = {}\n",
    "                for by_key in L_by_key:\n",
    "                    for by_value, subdf in df_history.groupby(by_key):\n",
    "                        for f_k, f_v in d_filter.items():\n",
    "                            sbdf = subdf[subdf[f_k]==f_v]\n",
    "                            for agg_key in L_agg_key:\n",
    "                                ser = sbdf[agg_key]\n",
    "                                d.update({f'{prefix}___filter_{f_k}__{by_key}_{by_value}__{agg_key}__mean' : ser.mean(),\n",
    "                                            f'{prefix}___filter_{f_k}__{by_key}_{by_value}__{agg_key}__sum' : ser.sum()})\n",
    "                return d\n",
    "\n",
    "    L_GLOBAL_KEYS= [\n",
    "        'id', 'match_id', 'match_type', 'number_of_games', 'date', 'year',\n",
    "        'month', 'day', 'weekday', 'hour', 'map_id', 'league_id', 'serie_id',\n",
    "        'tournament_id', 'serie_tier'\n",
    "    ]\n",
    "\n",
    "    L_FILTER_KEYS= [\n",
    "        'year','month', 'day', 'weekday', 'hour', \n",
    "        'league_id', 'serie_id', 'tournament_id', 'serie_tier'\n",
    "    ]\n",
    "\n",
    "    L_AGG_KEYS = [\n",
    "        'win', 'maxround', 'h1_win_count',\n",
    "        'h2_win_count', 'h1_outcome_eliminated_count',\n",
    "        'h1_outcome_defused_count', 'h1_outcome_timeout_count',\n",
    "        'h1_outcome_eliminated_count', 'h2_outcome_timeout_count',\n",
    "        'h2_outcome_defused_count', 'h2_outcome_exploded_count', 'adr',\n",
    "        'assists', 'deaths', 'first_kills_diff', 'flash_assists', 'headshots',\n",
    "        'k_d_diff', 'kast', 'kills', 'rating', 'h1_outcome_exploded_count'\n",
    "    ]\n",
    "    L_AGG_KEYS_V2 = [\n",
    "        'adr',\n",
    "        'assists', 'deaths', 'first_kills_diff', 'flash_assists', 'headshots',\n",
    "        'k_d_diff', 'kast', 'kills', 'rating'\n",
    "    ]\n",
    "    L_GROUP_KEYS= ['year','month', 'day','weekday','hour', 'serie_tier','number_of_games']\n",
    "    L_GROUP_KEYS_V2= ['year','month', 'day','weekday','hour']\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    for game_id in tqdm.tqdm(L_GAME_IDXS):\n",
    "\n",
    "        df_game = df_team_profile.query('id==@game_id')\n",
    "\n",
    "        d_fs4gm = df_game[L_GLOBAL_KEYS].iloc[0].to_dict()\n",
    "\n",
    "        date = df_game['date'].iloc[0]\n",
    "        map_id = df_game['map_id'].iloc[0]\n",
    "        d_filter = {\n",
    "            'league_id':df_game['league_id'].iloc[0],\n",
    "            'serie_id':df_game['serie_id'].iloc[0],\n",
    "            'tournament_id':df_game['tournament_id'].iloc[0]\n",
    "        }\n",
    "        \n",
    "        d_team_id2start_ct = dict(zip(df_game['team_id'],df_game['start_ct']))\n",
    "        d_team_id2opponent_id = dict(zip(df_game['team_id'],df_game['opponent_id']))\n",
    "        d_team_id2lineup = dict(zip(df_game['team_id'],df_game['lineup']))\n",
    "        d_team_id2loc = dict(zip(df_game['team_id'], df_game['team_location']))    \n",
    "\n",
    "        for team_id, start_ct in d_team_id2start_ct.items():\n",
    "\n",
    "            PREFIX = 'START_CT' if start_ct==1 else 'START_T'\n",
    "            \n",
    "            subdf_player_profile = df_player_profile.query('(id==@game_id)&(team_id==@team_id)')\n",
    "            L_ps = subdf_player_profile['player_id'].sort_values().values\n",
    "            d_player_id2nat = dict(zip(subdf_player_profile['player_id'], subdf_player_profile['player_nationality']))\n",
    "            d_player_id2birthday = dict(zip(subdf_player_profile['player_id'], subdf_player_profile['player_birthday']))\n",
    "\n",
    "            d_team_id2loc = dict(zip(df_game['team_id'], df_game['team_location']))\n",
    "            opponent_id = d_team_id2opponent_id[team_id]\n",
    "            lineup = d_team_id2lineup[team_id]\n",
    "\n",
    "            d_fs4gm[f'{PREFIX}__team_id'] = team_id\n",
    "            d_fs4gm[f'{PREFIX}__team_location'] = d_team_id2loc[team_id]\n",
    "            d_fs4gm[f'{PREFIX}__lineup'] = lineup\n",
    "\n",
    "            \n",
    "            df_history4team = df_team_profile.query('(date<@date)&(team_id==@team_id)')\n",
    "            df_history4team_with_map_and_start = df_history4team.query('(map_id==@map_id)&(start_ct==@start_ct)')\n",
    "            df_history4team_with_lineup = df_history4team.query('(lineup==@lineup)')\n",
    "            df_history4team_with_map_and_start_and_lineup = df_history4team_with_map_and_start.query('(lineup==@lineup)')\n",
    "            df_history4team_with_opponent = df_history4team.query('(opponent_id==@opponent_id)')\n",
    "            df_history4team_with_map_and_start_and_opponent = df_history4team_with_map_and_start.query('(opponent_id==@opponent_id)')\n",
    "            L_dataframes = [\n",
    "                df_history4team, df_history4team_with_map_and_start, df_history4team_with_lineup,\n",
    "                df_history4team_with_map_and_start_and_lineup, df_history4team_with_opponent,\n",
    "                df_history4team_with_map_and_start_and_opponent\n",
    "            ]\n",
    "            del df_history4team, df_history4team_with_map_and_start, df_history4team_with_lineup,\\\n",
    "                df_history4team_with_map_and_start_and_lineup, df_history4team_with_opponent,\\\n",
    "                df_history4team_with_map_and_start_and_opponent\n",
    "            L_prefix = [f'{PREFIX}__team__all_map_all_start', f'{PREFIX}__team__current_map_current_start',\n",
    "                        f'{PREFIX}__team__all_map_all_start_with_lineup', f'{PREFIX}__team__current_map_current_start_with_lineup',\n",
    "                        f'{PREFIX}__team__all_map_all_start__with_pair', f'{PREFIX}__team__current_map_current_start_with_pair']        \n",
    "            for prefix, df_history in zip(L_prefix, L_dataframes):        \n",
    "                d_fs4gm.update(add_features(df_history, L_by_key=None, d_filter = None,L_agg_key=L_AGG_KEYS, prefix= prefix))\n",
    "                d_fs4gm.update(add_features(df_history, L_by_key=L_GROUP_KEYS, d_filter = None, L_agg_key=L_AGG_KEYS, prefix= prefix))  \n",
    "                d_fs4gm.update(add_features(df_history, L_by_key=None, d_filter = d_filter, L_agg_key=L_AGG_KEYS, prefix= prefix))   \n",
    "            del L_dataframes\n",
    "\n",
    "            for i, player_id in enumerate(L_ps):\n",
    "                \n",
    "                d_fs4gm[f'{PREFIX}__player{i+1}_id'] = player_id\n",
    "                d_fs4gm[f'{PREFIX}_player{i+1}_nationality'] = d_player_id2nat[player_id]\n",
    "                d_fs4gm[f'{PREFIX}_player{i+1}_birthday'] = d_player_id2birthday[player_id]\n",
    "\n",
    "                df_history4player = df_player_profile.query('(date<@date)&(player_id==@player_id)')\n",
    "                df_history4player_in_team = df_history4player.query('team_id==@team_id')\n",
    "                df_history4player_not_in_team =  df_history4player.query('team_id!=@team_id')\n",
    "                del df_history4player\n",
    "\n",
    "                d_fs4gm.update(add_features(df_history4player_in_team, L_by_key=None, d_filter = None,L_agg_key=L_AGG_KEYS_V2, prefix= f'{PREFIX}__player{i+1}_in_team'))\n",
    "                d_fs4gm.update(add_features(df_history4player_not_in_team, L_by_key=None, d_filter = None, L_agg_key=L_AGG_KEYS_V2, prefix= f'{PREFIX}__player{i+1}_not_in_team')) \n",
    "                d_fs4gm.update(add_features(df_history4player_in_team, L_by_key=L_GROUP_KEYS_V2, d_filter = None,L_agg_key=L_AGG_KEYS_V2, prefix= f'{PREFIX}__player{i+1}_in_team'))\n",
    "                d_fs4gm.update(add_features(df_history4player_not_in_team, L_by_key=L_GROUP_KEYS_V2, d_filter = None, L_agg_key=L_AGG_KEYS_V2, prefix= f'{PREFIX}__player{i+1}_not_in_team')) \n",
    "\n",
    "        path_out = r'D:\\L_features_25042022\\{}.pickle'.format(int(game_id))\n",
    "        with open(path_out, 'wb') as f:\n",
    "            pickle.dump(d_fs4gm, f)\n",
    "        del d_fs4gm\n",
    "        \n",
    "    return True   \n",
    "\n",
    "def reduce_mem_usage(series):\n",
    "    try:\n",
    "        col_type = series.dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = series.min()\n",
    "            c_max = series.max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    series = series.astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    series = series.astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    series = series.astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    series = series.astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    series = series.astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    series = series.astype(np.float32)\n",
    "                else:\n",
    "                    series = series.astype(np.float64)\n",
    "        else:\n",
    "            pass \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return series \n",
    "\n",
    "def prepare_data(df_targets, df_features):\n",
    "\n",
    "    df_targets = df_targets.set_index('id').astype(int)\n",
    "    df_features = df_features.set_index('id').drop(['match_id', 'date', 'match_type'], 1)\n",
    "    games2use= np.intersect1d(df_features.index, df_targets.index)\n",
    "\n",
    "    X = df_features.loc[games2use]\n",
    "    del df_features\n",
    "    gc.collect()\n",
    "    Y = df_targets.loc[games2use]\n",
    "    del df_targets\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    L_BD_KEYS = [\n",
    "        'START_CT_player1_birthday', 'START_CT_player2_birthday',\n",
    "        'START_CT_player3_birthday', 'START_CT_player4_birthday',\n",
    "        'START_CT_player5_birthday', 'START_T_player1_birthday',\n",
    "        'START_T_player2_birthday', 'START_T_player3_birthday',\n",
    "        'START_T_player4_birthday', 'START_T_player5_birthday'\n",
    "    ]\n",
    "\n",
    "    X_bd= pd.concat([\n",
    "        X[L_BD_KEYS].astype('datetime64').apply(lambda x: x.dt.year).fillna(-9999).astype(int).astype('category').add_suffix('_year'),\n",
    "        X[L_BD_KEYS].astype('datetime64').apply(lambda x: x.dt.month).fillna(-9999).astype(int).astype('category').add_suffix('_month'),\n",
    "        X[L_BD_KEYS].astype('datetime64').apply(lambda x: x.dt.day).fillna(-9999).astype(int).astype('category').add_suffix('_day')\n",
    "        ],1)\n",
    "\n",
    "    X= pd.concat([X.drop(L_BD_KEYS, 1), X_bd], 1)\n",
    "\n",
    "    L_CAT_FEATURES_V1 = [\n",
    "        'START_CT__team_id', 'START_T__team_id', \n",
    "        'START_CT__player1_id', 'START_CT__player2_id',\n",
    "        'START_CT__player3_id', 'START_CT__player4_id', 'START_CT__player5_id',\n",
    "        'START_T__player1_id', 'START_T__player2_id',\n",
    "        'START_T__player3_id', 'START_T__player4_id', 'START_T__player5_id'\n",
    "    ]\n",
    "    X[L_CAT_FEATURES_V1] = X[L_CAT_FEATURES_V1].fillna(-9999).astype(int).astype('category')\n",
    "\n",
    "    L_CAT_FEATURES_V2 = [\n",
    "        'number_of_games', 'year', 'month', 'day', 'weekday', 'hour', 'map_id',\n",
    "        'league_id', 'serie_id', 'tournament_id']\n",
    "    X[L_CAT_FEATURES_V2] = X[L_CAT_FEATURES_V2].fillna(-9999).astype(int).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "    L_CAT_FEATURES_V3 = [\n",
    "        'START_CT__team_location', 'START_T__team_location', \n",
    "        'START_CT_player1_nationality', 'START_CT_player2_nationality',\n",
    "        'START_CT_player3_nationality', 'START_CT_player4_nationality',\n",
    "        'START_CT_player5_nationality', 'START_T_player1_nationality',\n",
    "        'START_T_player2_nationality', 'START_T_player3_nationality',\n",
    "        'START_T_player4_nationality', 'START_T_player5_nationality']\n",
    "    X[L_CAT_FEATURES_V3] = X[L_CAT_FEATURES_V3].fillna('default').astype('category')\n",
    "    X['serie_tier'] = X['serie_tier'].fillna('default').astype('category')\n",
    "\n",
    "    X[['START_CT__lineup', 'START_T__lineup']] = X[['START_CT__lineup', 'START_T__lineup']].fillna('default').astype('category')\n",
    "\n",
    "    # X_obj = X.select_dtypes('category').astype('object')\n",
    "    # L_obj_keys = X_obj.columns\n",
    "    # for cmb in itertools.combinations(L_obj_keys, 2):\n",
    "    #     cmb= list(cmb)\n",
    "    #     new_key = '-'.join([str(x) for x in cmb])    \n",
    "    #     X[new_key] = X_obj[cmb].astype('str').apply(lambda x: '-'.join(x), axis = 1).astype('category')\n",
    "    # del X_obj\n",
    "    # gc.collect()\n",
    "\n",
    "    return {'features':X, 'targets':Y}\n",
    "\n",
    "class FeatureSelector():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_tr, X_ho, y_tr, y_ho):\n",
    "\n",
    "        # оптимизация гиперпараметров\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params)        \n",
    "        model.fit(X_tr, y_tr, eval_set=(X_ho, y_ho), early_stopping_rounds=50)\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        best_i = model.best_iteration_\n",
    "        params['iterations'] = best_i\n",
    "        params['verbose'] =0\n",
    "        model = cb.CatBoostClassifier(**params)    \n",
    "\n",
    "        cb_opt = CatBoostOptimizer(\n",
    "                        scoring_func= lambda y, y_proba: roc_auc_score(y, y_proba),\n",
    "                        const_params=params,\n",
    "                        seed=SEED, \n",
    "                        direction='maximize',\n",
    "                        n_trials=30\n",
    "            )\n",
    "        cb_opt.fit(X_tr, y_tr)\n",
    "        best_params = cb_opt.transform()\n",
    "        params.update(best_params)\n",
    "        params['verbose'] = 0\n",
    "        params['random_state'] = SEED\n",
    "        best_params = params.copy()\n",
    "\n",
    "        # четвертая стадия отбора (важность признаковпри перемешивании)\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            print('> permutation importance iter#{}. n_features = {}'.format(i, X_tr.shape[1]))    \n",
    "\n",
    "            # ДО \n",
    "            params = best_params.copy()        \n",
    "            params['cat_features'] = np.where(X_tr.dtypes=='category')[0]   \n",
    "            params['iterations'] = best_i \n",
    "            model = cb.CatBoostClassifier(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            ho_score_before=roc_auc_score(y_ho, model.predict_proba(X_ho)[:, 1])        \n",
    "\n",
    "            # отбор\n",
    "            L_perm_imp = []\n",
    "            for j in tqdm.tqdm(range(10)):\n",
    "                d_perm_imp = permutation_importance(model, X_ho, y_ho, scoring='roc_auc', n_repeats=1, random_state = SEED+j, n_jobs=-1)\n",
    "                L_perm_imp.append(d_perm_imp['importances_mean'].flatten())\n",
    "            arr_perm_imp_mean =np.r_[L_perm_imp].mean(0)\n",
    "            # отобранные признаки\n",
    "            idx_selected = np.where(arr_perm_imp_mean>0)[0]\n",
    "\n",
    "            # ПОСЛЕ\n",
    "            params_c=params.copy()\n",
    "            params_c['cat_features'] = np.where(X_tr.iloc[:, idx_selected].dtypes=='category')[0]\n",
    "            params_c['iterations'] = best_i \n",
    "            model = cb.CatBoostClassifier(**params_c)\n",
    "            model.fit(X_tr.iloc[:, idx_selected], y_tr)            \n",
    "            ho_score_after = roc_auc_score(y_ho, model.predict_proba(X_ho.iloc[:, idx_selected])[:, 1])\n",
    "\n",
    "            # если метрика улучшилась, продолжаем \n",
    "            print('\\t> score before: {:.2f}, score after: {:.2f}'.format(ho_score_before, ho_score_after))\n",
    "            if ho_score_after > ho_score_before:\n",
    "                best_score = ho_score_after\n",
    "                best_features= X_tr.columns\n",
    "                best_params = params_c\n",
    "                X_tr, X_ho = X_tr.iloc[:, idx_selected], X_ho.iloc[:, idx_selected]\n",
    "                i+=1\n",
    "            #  если нет, останавливаемся\n",
    "            else:\n",
    "                break\n",
    "        self.best_features = X_tr.columns\n",
    "        self.best_params = best_params\n",
    "        self.X_tr, self.X_ho = X_tr, X_ho\n",
    "        del X_tr, X_ho\n",
    "        self.y_tr, self.y_ho = y_tr, y_ho\n",
    "        del y_tr, y_ho\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X_te, y_te):\n",
    "        model = cb.CatBoostClassifier(**fs.best_params)\n",
    "        X_trho = pd.concat([self.X_tr[fs.best_features], self.X_ho[fs.best_features]])\n",
    "        y_trho = pd.concat([self.y_tr, self.y_ho]) \n",
    "        assert (X_trho.index == y_trho.index).all()\n",
    "        X_te_c = X_te[fs.best_features]\n",
    "        model.fit(X_trho, y_trho)\n",
    "        return roc_auc_score(y_te, model.predict_proba(X_te_c)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/57188 [00:00<07:59, 119.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> collecting responses ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57188/57188 [00:32<00:00, 1749.06it/s]\n",
      "100%|██████████| 57187/57187 [00:00<00:00, 316796.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> preparing team/map to use ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 76/57187 [00:00<01:18, 725.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> preparing team/player profiles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57187/57187 [04:11<00:00, 227.29it/s]\n",
      "100%|██████████| 69256/69256 [02:53<00:00, 399.18it/s]\n",
      "100%|██████████| 69256/69256 [01:42<00:00, 672.97it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> extracting features for new games ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> preparing features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 17/70 [09:51<32:39, 36.97s/it]"
     ]
    }
   ],
   "source": [
    "# директория с коллекцией респонсов\n",
    "PATH_TO_RESPONSES = 'L_games_collection'\n",
    "# директория с признаками\n",
    "PATH_TO_FEATURES = r'D:\\L_features_25042022'\n",
    "L_FILENAMES = os.listdir(PATH_TO_FEATURES)\n",
    "# размер батча подготовки признаков\n",
    "BATCH_SIZE = 500\n",
    "# батчи признаков\n",
    "L_BATCHES = np.array_split(L_FILENAMES, np.int32(np.ceil(len(L_FILENAMES)/BATCH_SIZE)))\n",
    "# итерации бустинга\n",
    "CONST_PARAMS= {\n",
    "    'iterations':1000,\n",
    "    'loss_function':'Logloss',    \n",
    "    'verbose':1,\n",
    "}\n",
    "# сид рандома\n",
    "SEED = 13\n",
    "# доля тестовой части\n",
    "TEST_SIZE= .05\n",
    "# доля отложенной части\n",
    "HOLD_SIZE = .1\n",
    "# размер батча \n",
    "BATCH_RATE = .05\n",
    "\n",
    "maps_str= \"\"\"\n",
    "Vertigo\n",
    "Inferno\n",
    "Nuke\n",
    "Dust2\n",
    "Mirage \n",
    "Ancient \n",
    "Overpass\n",
    "\"\"\"\n",
    "\n",
    "teams_str=\\\n",
    "\"\"\"\n",
    "Natus Vincere\n",
    "Gambit\n",
    "NIP\n",
    "Vitality\n",
    "G2\n",
    "FaZe\n",
    "Heroic\n",
    "Astralis\n",
    "Virtus.pro\n",
    "OG\n",
    "ENCE\n",
    "BIG\n",
    "Liquid\n",
    "Movistar Riders\n",
    "Copenhagen Flames\n",
    "FURIA\n",
    "mousesports\n",
    "forZe\n",
    "Spirit\n",
    "Entropiq\n",
    "Complexity\n",
    "Sinners\n",
    "Fiend\n",
    "SKADE\n",
    "GODSENT\n",
    "fnatic\n",
    "Lyngby Vikings\n",
    "DBL PONEY\n",
    "paiN\n",
    "Dignitas\n",
    "Bad News Bears\n",
    "Evil Geniuses\n",
    "TeamOne\n",
    "Sharks\n",
    "00Nation\n",
    "Bravos\n",
    "Havan Liberty\n",
    "MIBR\n",
    "FATE\n",
    "eSuba\n",
    "ECLOT\n",
    "Entropiq Prague\n",
    "OPAA\n",
    "AaB\n",
    "MASONIC\n",
    "Tricked\n",
    "AGF\n",
    "Astralis Talent\n",
    "HAVU\n",
    "KOVA\n",
    "hREDS\n",
    "SJ\n",
    "LDLC\n",
    "Sprout\n",
    "cowana\n",
    "NLG\n",
    "TTC\n",
    "BIG Academy\n",
    "AGO\n",
    "Wisla Krakow\n",
    "Anonymo\n",
    "Izako Boars\n",
    "HONORIS\n",
    "PACT\n",
    "sAw\n",
    "SAW Youngsters\n",
    "FTW\n",
    "OFFSET\n",
    "Nexus\n",
    "ONYX\n",
    "4glory\n",
    "Enterprise\n",
    "GamerLegion\n",
    "Galaxy Racer\n",
    "Apeks\n",
    "AURA\n",
    "Young Ninjas\n",
    "Lilmix\n",
    "Eternal Fire\n",
    "Sangal\n",
    "Endpoint\n",
    "1WIN\n",
    "K23\n",
    "INDE IRAE\n",
    "AVE\n",
    "Singularity\n",
    "NAVI Junior\n",
    "Spirit Academy\n",
    "VP.Pridigy\n",
    "Trasko\n",
    "EC Kyiv\n",
    "B8\n",
    "TyLoo\n",
    "ViCi\n",
    "Lynn Vision\n",
    "Invictus\n",
    "Checkmate\n",
    "D13\n",
    "Renegades\n",
    "\"\"\"\n",
    "#############################################################################################################\n",
    "\n",
    "time.sleep(1)\n",
    "print('> collecting responses ...')\n",
    "# коллекция респонсов\n",
    "L_COLLECTION = get_game_collection(PATH_TO_RESPONSES)\n",
    "\n",
    "time.sleep(1)\n",
    "print('> preparing team/map to use ...')\n",
    "d_map_id2name = {}\n",
    "d_team_id2name = {}\n",
    "for d_rsp in tqdm.tqdm(L_COLLECTION):\n",
    "    try:\n",
    "        d_map_id2name[d_rsp['map']['id']] = str.lower(d_rsp['map']['name']).strip()\n",
    "        for t in d_rsp['teams']:\n",
    "            d_team_id2name[t['id']] = str.lower(t['name']).strip()\n",
    "    except:\n",
    "        pass\n",
    "d_map_name2id={v:k for k, v in d_map_id2name.items()}\n",
    "d_team_name2id={v:k for k, v in d_team_id2name.items()}\n",
    "L_maps2use = [str.lower(x.strip()) for x in maps_str.split('\\n')][1:-1]\n",
    "L_map_id2use = [d_map_name2id[map_name] for map_name in L_maps2use]\n",
    "L_teams2use = [str.lower(x.strip()) for x in teams_str.split('\\n')][1:-1]\n",
    "L_team_id2use = []\n",
    "for team_name in L_teams2use:\n",
    "    if team_name in d_team_name2id.keys():\n",
    "        L_team_id2use.append(d_team_name2id[team_name])\n",
    "\n",
    "time.sleep(1)\n",
    "print('> preparing team/player profiles ...')\n",
    "# профайлинг игроков и команд в играх\n",
    "d_profile = get_profiles(L_COLLECTION)\n",
    "df_player_profile, df_team_profile = d_profile['player'], d_profile['team']\n",
    "\n",
    "time.sleep(1)\n",
    "print('> extracting features for new games ...')\n",
    "# идентификаторы игр\n",
    "L_GAME_IDXS = np.unique(df_player_profile['id'])[::-1]\n",
    "# обработанные игры\n",
    "s_in=set([int(x.split('.')[0]) for x in L_FILENAMES])\n",
    "# все игры\n",
    "s_all= set(L_GAME_IDXS)\n",
    "# новые игры\n",
    "s_new= s_all-s_in\n",
    "# идентификаторы новых игр\n",
    "L_GAME_IDXS = list(s_new)\n",
    "# создание признаков\n",
    "get_features(df_player_profile, L_GAME_IDXS)\n",
    "\n",
    "time.sleep(1)\n",
    "print('> preparing features ...')\n",
    "# батчи признаков\n",
    "L_BATCHES = np.array_split(L_FILENAMES, np.int32(np.ceil(len(L_FILENAMES)/BATCH_SIZE)))\n",
    "# таблица с признаками\n",
    "df_features = pd.DataFrame()\n",
    "for batch in  tqdm.tqdm(L_BATCHES):\n",
    "    L = []\n",
    "    for fnm in batch:\n",
    "        pth = os.path.join(PATH_TO_FEATURES, fnm)\n",
    "        with open(pth, 'rb') as f:\n",
    "            d = pickle.load(f)\n",
    "        L.append(d)\n",
    "        del d\n",
    "    df = pd.DataFrame.from_records(L).apply(reduce_mem_usage)\n",
    "    del L\n",
    "    df_features = df_features.append(df)\n",
    "    del df\n",
    "df_features.to_pickle('df_features.pickle')\n",
    "\n",
    "time.sleep(1)\n",
    "print('> preparing targets ...')\n",
    "# игры \n",
    "L_GAMES2USE = np.unique(df_features['id'])\n",
    "# таблица с целевыми переменными\n",
    "df_targets = get_targets(L_COLLECTION, L_GAMES2USE)\n",
    "\n",
    "time.sleep(1)\n",
    "print('> preparing dataset ...')\n",
    "# подготовка датасета \n",
    "d_data = prepare_data(df_targets, df_features)\n",
    "del df_features, df_targets\n",
    "gc.collect()\n",
    "X, Y = d_data['features'], d_data['targets']\n",
    "del d_data\n",
    "gc.collect()\n",
    "X.to_pickle('X.pickle'), Y.to_pickle('Y.pickle')\n",
    "#############################################################################################################\n",
    "\n",
    "# датасет\n",
    "X, Y = pd.read_pickle('X.pickle'), pd.read_pickle('Y.pickle')\n",
    "X_num = X.select_dtypes('number').fillna(-9999)\n",
    "X_cat = X.select_dtypes(exclude = ['number'])\n",
    "del X\n",
    "gc.collect()\n",
    "X = pd.concat([X_cat, X_num], 1)\n",
    "del X_cat, X_num\n",
    "gc.collect()\n",
    "#############################################################################################################\n",
    "\n",
    "# выполенеие пайплайна для целевых переменных\n",
    "for target_i, target_key in enumerate(Y.columns):\n",
    "    \n",
    "    time.sleep(1)\n",
    "    print('> target#{}/{}. {}'.format(target_i+1, Y.shape[1], target_key))  \n",
    "\n",
    "    try:\n",
    "\n",
    "        # целевая переменная\n",
    "        y= Y[target_key]\n",
    "\n",
    "        # сплит\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, shuffle =False, test_size = TEST_SIZE)\n",
    "        X_tr, X_ho, y_tr, y_ho = train_test_split(X_tr, y_tr, shuffle =False, test_size = HOLD_SIZE)\n",
    "        assert (X_tr.index==y_tr.index).all()\n",
    "        gc.collect()\n",
    "\n",
    "        # первая стадия отбора признаков\n",
    "        while True:\n",
    "\n",
    "            # число строк\n",
    "            n_rows = X_tr.shape[0]\n",
    "            # число столбцов\n",
    "            n_cols= X_tr.shape[1]\n",
    "            # бъем столбцы на батчи по 10% от исходного числа столбцов\n",
    "            col_batch_size = np.int32(np.ceil(n_rows*BATCH_RATE))\n",
    "            n_batches = np.int32(np.ceil(n_cols/col_batch_size))\n",
    "            L_FEATURE_BATCHES = np.array_split(X_tr.columns, n_batches)\n",
    "\n",
    "            if len(L_FEATURE_BATCHES)==1:\n",
    "                break\n",
    "            else:\n",
    "\n",
    "                # отобранные признаки\n",
    "                L_feat2use =[]\n",
    "                for i, batch in enumerate(L_FEATURE_BATCHES[::-1]):\n",
    "\n",
    "                    print('> batch#{}/{}'.format(i+1, len(L_FEATURE_BATCHES)))\n",
    "                    \n",
    "                    # батч признаков\n",
    "                    x_tr_batch, x_ho_batch= X_tr[batch], X_ho[batch]\n",
    "                    \n",
    "                    # модель\n",
    "                    params = CONST_PARAMS.copy()\n",
    "                    params['cat_features'] = np.where(x_tr_batch.dtypes=='category')[0]\n",
    "                    params['thread_count'] = -1\n",
    "                    model = cb.CatBoostClassifier(**params)  \n",
    "                        \n",
    "                    # обучение\n",
    "                    model.fit(x_tr_batch, y_tr, eval_set=(x_ho_batch, y_ho), early_stopping_rounds=50)\n",
    "\n",
    "                    # отобранные признаки\n",
    "                    mask = model.feature_importances_>0\n",
    "                    \n",
    "                    # добавление отобранных признаков\n",
    "                    L_feat2use.extend(batch[mask].tolist())\n",
    "                    del x_tr_batch, x_ho_batch  \n",
    "\n",
    "                X_tr_c, X_ho_c = X_tr[L_feat2use], X_ho[L_feat2use]\n",
    "                del X_tr, X_ho\n",
    "                X_tr, X_ho = X_tr_c, X_ho_c\n",
    "                del X_tr_c, X_ho_c\n",
    "                gc.collect()\n",
    "\n",
    "        # вторая стадия отбра признаков (рекурсивный отбор с ранней остановкой)\n",
    "        i = 1\n",
    "        while True:\n",
    "\n",
    "            print('> iter#{}'.format(i))\n",
    "\n",
    "            # параметры\n",
    "            params = CONST_PARAMS.copy()\n",
    "            params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "\n",
    "            # модель\n",
    "            model = cb.CatBoostClassifier(**params)        \n",
    "            \n",
    "            # обучение\n",
    "            model.fit(X_tr, y_tr, eval_set=(X_ho, y_ho), early_stopping_rounds=50)\n",
    "\n",
    "            # маска отборанных признаков\n",
    "            mask = model.feature_importances_>0\n",
    "\n",
    "            # если все признаки отобраны, останавливаемся\n",
    "            if np.all(mask):\n",
    "                break\n",
    "\n",
    "            # если нет, обновляем признаки, повторяем\n",
    "            else:\n",
    "                X_tr_c, X_ho_c = X_tr.loc[:, mask], X_ho.loc[:, mask]\n",
    "                del X_tr, X_ho\n",
    "                X_tr, X_ho = X_tr_c, X_ho_c\n",
    "                del X_tr_c, X_ho_c\n",
    "                best_features = X_tr.columns\n",
    "                i+=1\n",
    "                \n",
    "        X_te = X_te[best_features]\n",
    "        with open('best_features.pickle', 'wb') as f:\n",
    "            pickle.dump(best_features, f)\n",
    "\n",
    "        # добавление признаков\n",
    "        # 1. комбинации для категорий\n",
    "        # 2. бинаризация для чисел\n",
    "\n",
    "        # категории\n",
    "        X_cat_tr = X_tr.select_dtypes('category').astype('object')\n",
    "        cat_features = X_cat_tr.columns\n",
    "        # числа\n",
    "        X_num_tr = X_tr.drop(X_cat_tr.columns, 1)\n",
    "        num_features = X_num_tr.columns\n",
    "\n",
    "        X_cat_ho = X_ho[cat_features].astype('object')\n",
    "        X_num_ho = X_ho[num_features]\n",
    "        X_cat_te = X_te[cat_features].astype('object')\n",
    "        X_num_te = X_te[num_features]\n",
    "\n",
    "        # комбинации из 2ух, 3ех\n",
    "        for i in tqdm.tqdm([2, 3]):\n",
    "            for cmb in itertools.combinations(cat_features, i):\n",
    "                cmb = list(cmb)\n",
    "                new_key = '-'.join([str(x) for x in cmb])\n",
    "                X_cat_tr[new_key] = X_cat_tr[cmb].astype(str).apply(lambda x: '-'.join(x), axis = 1)\n",
    "                X_cat_ho[new_key] = X_cat_ho[cmb].astype(str).apply(lambda x: '-'.join(x), axis = 1)\n",
    "                X_cat_te[new_key] = X_cat_te[cmb].astype(str).apply(lambda x: '-'.join(x), axis = 1)\n",
    "\n",
    "        # бинаризация чисел\n",
    "        # бакеты\n",
    "        L_percentile = np.around(np.linspace(2.5, 97.5, 10), 1)\n",
    "        # для каждого числового признака\n",
    "        for key in tqdm.tqdm(X_num_tr.columns):\n",
    "            \n",
    "            # тренировочная, отложенная, тестовая части\n",
    "            ser_tr, ser_ho, ser_te = X_num_tr[key], X_num_ho[key], X_num_te[key]\n",
    "            \n",
    "            # бины\n",
    "            bins = np.percentile(ser_tr, L_percentile)\n",
    "            \n",
    "            # бинаризация (l1-норма между значением и бинами)\n",
    "            x_bin_tr = ser_tr.apply(lambda x: np.abs(x-bins).argmin())\n",
    "            x_bin_ho = ser_ho.apply(lambda x: np.abs(x-bins).argmin())\n",
    "            x_bin_te = ser_te.apply(lambda x: np.abs(x-bins).argmin())\n",
    "            \n",
    "            # добавление нового признака\n",
    "            X_num_tr[f'{key}_bin'] = x_bin_tr.astype('category')\n",
    "            X_num_ho[f'{key}_bin'] = x_bin_ho.astype('category')\n",
    "            X_num_te[f'{key}_bin'] = x_bin_te.astype('category')\n",
    "            del x_bin_tr, x_bin_ho, x_bin_te\n",
    "\n",
    "        X_tr = pd.concat([X_cat_tr.astype('category'), X_num_tr], 1)\n",
    "        X_ho = pd.concat([X_cat_ho.astype('category'), X_num_ho], 1)\n",
    "        X_te = pd.concat([X_cat_te.astype('category'), X_num_te], 1)\n",
    "\n",
    "        # третья стадия отбра признаков (рекурсивный отбор с ранней остановкой)\n",
    "        i = 1\n",
    "        while True:\n",
    "\n",
    "            print('> iter#{}'.format(i))\n",
    "\n",
    "            params = CONST_PARAMS.copy()\n",
    "            params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "\n",
    "            model = cb.CatBoostClassifier(**params)        \n",
    "            model.fit(X_tr, y_tr, eval_set=(X_ho, y_ho), early_stopping_rounds=50)\n",
    "\n",
    "            mask = model.feature_importances_>0\n",
    "            if np.all(mask):\n",
    "                break\n",
    "            else:\n",
    "                X_tr_c, X_ho_c = X_tr.loc[:, mask], X_ho.loc[:, mask]\n",
    "                del X_tr, X_ho\n",
    "                X_tr, X_ho = X_tr_c, X_ho_c\n",
    "                del X_tr_c, X_ho_c\n",
    "                best_features = X_tr.columns\n",
    "                i+=1\n",
    "                \n",
    "        X_te = X_te[best_features]\n",
    "        with open('best_features.pickle', 'wb') as f:\n",
    "            pickle.dump(best_features, f)\n",
    "\n",
    "        # оптимизация гиперпараметров\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params)        \n",
    "        model.fit(X_tr, y_tr, eval_set=(X_ho, y_ho), early_stopping_rounds=50)\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        params['iterations'] = model.best_iteration_\n",
    "        params['verbose'] =0\n",
    "        model = cb.CatBoostClassifier(**params)    \n",
    "\n",
    "        cb_opt = CatBoostOptimizer(\n",
    "                        scoring_func= lambda y, y_proba: roc_auc_score(y, y_proba),\n",
    "                        const_params=params,\n",
    "                        seed=SEED, \n",
    "                        direction='maximize',\n",
    "                        n_trials=30\n",
    "            )\n",
    "        cb_opt.fit(X_tr, y_tr)\n",
    "        best_params = cb_opt.transform()\n",
    "        params.update(best_params)\n",
    "        params['verbose'] = 0\n",
    "        params['random_state'] = SEED\n",
    "        best_params = params.copy()\n",
    "\n",
    "        # четвертая стадия отбора (важность признаковпри перемешивании)\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            print('> permutation importance iter#{}. n_features = {}'.format(i, X_tr.shape[1]))    \n",
    "\n",
    "            # ДО \n",
    "            params = best_params.copy()        \n",
    "            params['cat_features'] = np.where(X_tr.dtypes=='category')[0]    \n",
    "            model = cb.CatBoostClassifier(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            ho_score_before=roc_auc_score(y_ho, model.predict_proba(X_ho)[:, 1])\n",
    "            te_score_before=roc_auc_score(y_te, model.predict_proba(X_te)[:, 1])\n",
    "\n",
    "            # отбор\n",
    "            L_perm_imp = []\n",
    "            for j in tqdm.tqdm(range(20)):\n",
    "                d_perm_imp = permutation_importance(model, X_ho, y_ho, scoring='roc_auc', n_repeats=1, random_state = SEED+j, n_jobs=-1)\n",
    "                L_perm_imp.append(d_perm_imp['importances_mean'].flatten())\n",
    "            arr_perm_imp_mean =np.r_[L_perm_imp].mean(0)\n",
    "            # отобранные признаки\n",
    "            idx_selected = np.where(arr_perm_imp_mean>0)[0]\n",
    "\n",
    "            # ПОСЛЕ\n",
    "            params_c=params.copy()\n",
    "            params_c['cat_features'] = np.where(X_tr.iloc[:, idx_selected].dtypes=='category')[0]\n",
    "            model = cb.CatBoostClassifier(**params_c)\n",
    "            model.fit(X_tr.iloc[:, idx_selected], y_tr)\n",
    "            params_c['iterations'] = model.best_iteration_\n",
    "            ho_score_after = roc_auc_score(y_ho, model.predict_proba(X_ho.iloc[:, idx_selected])[:, 1])\n",
    "            te_score_after = roc_auc_score(y_te, model.predict_proba(X_te.iloc[:, idx_selected])[:, 1])\n",
    "\n",
    "            # если метрика улучшилась, продолжаем \n",
    "            print('\\t> score before: {:.2f}, score after: {:.2f}'.format(ho_score_before, ho_score_after))\n",
    "            if ho_score_after > ho_score_before:\n",
    "                best_score = ho_score_after\n",
    "                best_features= X_tr.columns\n",
    "                X_tr, X_ho, X_te = X_tr.iloc[:, idx_selected], X_ho.iloc[:, idx_selected], X_te.iloc[:, idx_selected]\n",
    "                i+=1\n",
    "            #  если нет, останавливаемся\n",
    "            else:\n",
    "                break\n",
    "        with open('best_features.pickle', 'wb') as f:\n",
    "            pickle.dump(best_features, f)\n",
    "\n",
    "        # оптимизация гиперпараметров\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        model = cb.CatBoostClassifier(**params)        \n",
    "        model.fit(X_tr, y_tr, eval_set=(X_ho, y_ho), early_stopping_rounds=50)\n",
    "        params = CONST_PARAMS.copy()\n",
    "        params['cat_features'] = np.where(X_tr.dtypes=='category')[0]\n",
    "        params['iterations'] = model.best_iteration_\n",
    "        params['verbose'] =0\n",
    "        model = cb.CatBoostClassifier(**params)    \n",
    "\n",
    "        cb_opt = CatBoostOptimizer(\n",
    "                        scoring_func= lambda y, y_proba: roc_auc_score(y, y_proba),\n",
    "                        const_params=params,\n",
    "                        seed=SEED, \n",
    "                        direction='maximize',\n",
    "                        n_trials=50\n",
    "            )\n",
    "        cb_opt.fit(X_tr, y_tr)\n",
    "        best_params = cb_opt.transform()\n",
    "        params.update(best_params)\n",
    "        params['verbose'] = 0\n",
    "        params['random_state'] = SEED\n",
    "        best_params = params.copy()\n",
    "\n",
    "        model = cb.CatBoostClassifier(**best_params) \n",
    "        X_trho = pd.concat([X_tr, X_ho], 0)\n",
    "        y_trho= y.loc[X_trho.index]\n",
    "\n",
    "        with open('L_res.pickle', 'wb') as f:\n",
    "            pickle.dump([X_trho, y_trho, X_te, y_te, model], f)\n",
    "\n",
    "        # ансамбль\n",
    "        X_tr, y_tr = X_trho, y_trho\n",
    "        del X_trho, y_trho\n",
    "        gc.collect()\n",
    "        params = model.get_params()\n",
    "\n",
    "        # модели ансамбля\n",
    "        L_models = []\n",
    "        for i in range(20):\n",
    "            params_c = params.copy()\n",
    "            params_c['random_state'] = SEED+i\n",
    "            model = cb.CatBoostClassifier(**params_c)\n",
    "            L_models.append(model)\n",
    "\n",
    "        # точность моделей\n",
    "        L_te_probas = []\n",
    "        for model in tqdm.tqdm(L_models):\n",
    "            model.fit(X_tr, y_tr)\n",
    "            te_proba = model.predict_proba(X_te)[:, 1].flatten()\n",
    "            L_te_probas.append(te_proba)\n",
    "        y_te_probas = np.c_[L_te_probas].T\n",
    "        for i in range(y_te_probas.shape[1]):\n",
    "            print('> model#{}. score = {}'.format(i+1, roc_auc_score(y_te, y_te_probas[:,i])))\n",
    "\n",
    "        # смешиваем комбинации от 1 до 10 моделей\n",
    "        n_models = y_te_probas.shape[1]\n",
    "        best_score = -np.inf\n",
    "        for i in range(1, 11):\n",
    "            for cmb in itertools.combinations(np.arange(n_models), i):\n",
    "                cmb=list(cmb)\n",
    "                score = roc_auc_score(y_te, y_te_probas[:, cmb].mean(1)) \n",
    "                if score > best_score:\n",
    "                    best_score=score\n",
    "                    best_cmb = cmb\n",
    "                    ensemble = np.array(L_models)[cmb]\n",
    "        print('> ensemble. model count: {}, score = {}'.format(len(ensemble), best_score))\n",
    "\n",
    "        # весь датасет\n",
    "        X_full = pd.concat([X_tr, X_te], 0)\n",
    "        y_full = y.loc[X_full.index]\n",
    "        del X_tr, X_te\n",
    "        # обученные модели\n",
    "        L_fitted_models = []\n",
    "        for model in tqdm.tqdm(ensemble):\n",
    "            model.fit(X_full, y_full)\n",
    "            L_fitted_models.append(model)    \n",
    "        # признаки\n",
    "        L_all_features = X_full.columns\n",
    "        L_fs4ct, L_fs4t = [], []\n",
    "        for f in L_all_features:\n",
    "            if 'START_CT' in f:\n",
    "                L_fs4ct.append(f)\n",
    "            else:\n",
    "                L_fs4t.append(f)\n",
    "        assert (len(L_fs4t)+len(L_fs4ct))==len(L_all_features)\n",
    "\n",
    "        #############################################################################################################\n",
    "        # подготовка предсказаний\n",
    "        df_data4model = X[['map_id', 'START_CT__team_id', 'START_T__team_id']].sort_index()\n",
    "\n",
    "        L_answers = []\n",
    "        for map_id in tqdm.tqdm(L_map_id2use):\n",
    "            subdf = df_data4model.query('map_id==@map_id')\n",
    "            for i in tqdm.tqdm(range(len(L_team_id2use))):\n",
    "                for j in range(i+1, len(L_team_id2use)):\n",
    "                    team1_id, team2_id= L_team_id2use[i], L_team_id2use[j]\n",
    "                    \n",
    "                    subdf_ct = subdf.query('START_CT__team_id==@team1_id')\n",
    "                    subdf_t = subdf.query('START_T__team_id==@team2_id')\n",
    "                    if (len(subdf_ct)!=0)&(len(subdf_t)!=0):\n",
    "                        idx4ct = subdf_ct.index[-1]\n",
    "                        idx4t = subdf_t.index[-1]\n",
    "                        d_fs4gm = X_full.loc[idx4ct, L_fs4ct].to_dict()\n",
    "                        d_fs4gm.update(X_full.loc[idx4t, L_fs4t].to_dict())\n",
    "                        x_new = pd.DataFrame.from_records([d_fs4gm])[L_all_features]\n",
    "                        proba = np.mean([model.predict_proba(x_new)[0][1] for model in L_fitted_models])\n",
    "                        d = {\n",
    "                            'map':d_map_id2name[map_id],\n",
    "                            'start_ct':d_team_id2name[team1_id], \n",
    "                            'start_t':d_team_id2name[team2_id],\n",
    "                            'start_ct_win_proba':proba\n",
    "                            }\n",
    "                        L_answers.append(d)\n",
    "\n",
    "                    subdf_ct = subdf.query('START_CT__team_id==@team2_id')\n",
    "                    subdf_t = subdf.query('START_T__team_id==@team1_id')\n",
    "                    if (len(subdf_ct)!=0)&(len(subdf_t)!=0):\n",
    "                        idx4ct = subdf_ct.index[-1]\n",
    "                        idx4t = subdf_t.index[-1]\n",
    "                        d_fs4gm = X_full.loc[idx4ct, L_fs4ct].to_dict()\n",
    "                        d_fs4gm.update(X_full.loc[idx4t, L_fs4t].to_dict())\n",
    "                        x_new = pd.DataFrame.from_records([d_fs4gm])[L_all_features]\n",
    "                        proba = np.mean([model.predict_proba(x_new)[0][1] for model in L_fitted_models])\n",
    "                        d = {\n",
    "                            'map':d_map_id2name[map_id],\n",
    "                            'start_ct':d_team_id2name[team2_id], \n",
    "                            'start_t':d_team_id2name[team1_id],\n",
    "                            'start_ct_win_proba':proba\n",
    "                            }\n",
    "                        L_answers.append(d)    \n",
    "        df_answers = pd.DataFrame.from_records(L_answers)\n",
    "        del L_answers\n",
    "        gc.collect()\n",
    "        df_answers.to_csv(r'C:\\Users\\Sergey\\anaconda3\\Scripts\\answers\\df_answers_{}.txt'.format(target_key))\n",
    "\n",
    "        del df_answers, X_full, y_full, L_fitted_models\n",
    "        gc.collect()\n",
    "\n",
    "        break\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc9092704b76bcd3825fbb7a05cb662b50152522ace34f313990aca7aedee1c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
